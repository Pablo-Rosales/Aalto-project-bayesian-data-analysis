---
title: "Project work"
author: "Anonymous"
date: "11/14/2020"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '1'
    df_print: paged
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loaded packages

```{r, message=FALSE, warning=FALSE}
library(rstan) 
rstan_options(auto_write = TRUE)
options(mc.cores = 3)
library(ggplot2)
library(aaltobda)
library(shinystan)
library(bayesplot)
library(loo)
library(dlookr)
library(corrplot)
library(rstanarm)
library(projpred)
library(GGally)
library(shiny)
library(gridExtra)
SEED <- 48927
```

# Data
## Breast cancer dataset
Mini introduccion
```{r}
cancer_data = read.csv('cancer.csv')
head(cancer_data)
```


Additionally, the data used has a total of 33 features (including the diagnosis, in this case, our target), and 569 samples for each of the features.
```{r}
print(nrow(cancer_data))
print(ncol(cancer_data))
```

# Preprocessing

## Initial checkup
The dataset has a column comprised of only NULL/NA values, so it will be dropped.
```{r}
any(is.na(cancer_data))
sum(is.na(cancer_data))

head(cancer_data[,33])
cancer_data$X <- NULL

any(is.na(cancer_data))
sum(is.na(cancer_data))
```

## Initial cleaning up
Como los datos estan presentados no se como, se limpia inicialmente de esta forma
```{r}
cancer_data[cancer_data == "B"] <- as.numeric(0)
cancer_data[cancer_data == "M"] <- as.numeric(1)
cancer_data$diagnosis <- as.numeric(as.character(cancer_data$diagnosis))

corrplot(cor(cancer_data[,2:32]))
describe(cancer_data)
head(cancer_data)
```

## Feature selection
For this task, we reduce the number of input variables in order to develop our models. As seen in the correlation map, variables such as **radius_mean**, **perimeter_mean** and **area_mean** are highly correlated. Thus, we discard the variables **radius_mean**, **perimeter_mean**, etc...

```{r}
cancer_data$radius_mean <- NULL
cancer_data$perimeter_mean <- NULL
cancer_data$compactness_mean <- NULL
cancer_data$concave.points_mean <- NULL
cancer_data$radius_se <- NULL
cancer_data$perimeter_se <- NULL
cancer_data$compactness_se <- NULL
cancer_data$concave.points_se <- NULL
cancer_data$radius_worst <- NULL
cancer_data$perimeter_worst <- NULL
cancer_data$compactness_worst <- NULL
cancer_data$concave.points_worst <- NULL
```

## Selected variables
The final variables, along with the correlation, used for our models are as follows:
```{r}
corrplot(cor(cancer_data))
```

## Scaling
The data has been scaled before being passed to the models.
```{r}
scaled_cancer_data <- cancer_data
col_names <- colnames(cancer_data)

scaled_cancer_data[,1:2] <- cancer_data[,1:2]
scaled_cancer_data[,3:18] <- scale(cancer_data[,3:18])
colnames(scaled_cancer_data) <- col_names
head(scaled_cancer_data)
```

# Linear Model
```{r}
writeLines(readLines("models/linear_model_bernoulli.stan"))

linear_model <- rstan::stan_model(file = "models/linear_model_bernoulli.stan")
```

## Linear Model - Sensitivity Analysis

```{r}
print("sensitivity analysis here")
```

## Linear Model - Evaluation

```{r}
variables <- c("texture_mean", "area_mean", "smoothness_mean", "concavity_mean",
               "symmetry_mean", "fractal_dimension_mean", "texture_se",
               "area_se", "smoothness_se", "concavity_se", "symmetry_se",
               "fractal_dimension_se", "texture_worst", "area_worst", 
               "smoothness_worst", "concavity_worst", "symmetry_worst", 
               "fractal_dimension_worst")

linear_models <- c()
N <- length(scaled_cancer_data$diagnosis)
y <- c(scaled_cancer_data$diagnosis)

for (variable in variables){
  cat("Sampling with variable: ", variable, "\n")
  linear_model_data <- list(N=N, y=y, x=c(scaled_cancer_data[,variable]))
  linear_sampling <- rstan::sampling(linear_model, data = linear_model_data,
                                     seed = SEED, refresh=0)
  linear_models <- c(linear_models, linear_sampling)
}

```

## Linear Model - Diagnostics and Comparisons
Using Leave-One-Out Cross Validation, we obtain the PSIS-LOO estimates for each of the variables for the Linear Model for posterior model comparison.

For the Linear Model itself, most of the models have exceptional k-values (lower than 0.7), which means that the results obtained by the models are reliable. Additionally by looking at the `elpd_loo` and `p_loo` values for each of the models, we can deduce that the best variables to be used in order for diagnosis are as follows (in order of best): `area_worst`, `area_mean`, `area_se`, `concavity_mean`.
```{r}
pareto_k_values <- list()
linear_loo_estimates_df <- data.frame()

for (i in (1:length(linear_models))){
  cat("- Monitoring variable: ", variables[i], "\n")
  model <- linear_models[[i]]
  model_log_lik <- extract_log_lik(model, parameter_name = "log_lik", merge_chains = FALSE)
  model_r_eff <- relative_eff(exp(model_log_lik), cores = 2)
  model_loo <- loo(model_log_lik, r_eff = model_r_eff, cores = 2)
  pareto_k_values <- append(pareto_k_values, list(model_loo$diagnostics$pareto_k))
  
  aux_df <- data.frame("variable" = variables[i], "elpd_loo" = model_loo$estimates[1], "p_loo" = model_loo$estimates[2])
  linear_loo_estimates_df <- rbind(linear_loo_estimates_df, aux_df)
}

cat("-- Linear Model Comparison")
print(linear_loo_estimates_df)
```

## Linear Model - Predictive Checking
Text aqui...
```{r}
y_sims <- list()
for (model in linear_models){
  params <- extract(model)
  y_sims <- append(y_sims, list(params$y_sim[1:100,]))
}

ppc1 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[1]])
ppc2 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[2]])
ppc3 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[3]])
ppc4 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[4]])
ppc5 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[5]])
ppc6 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[6]])
ppc7 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[7]])
ppc8 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[8]])
ppc9 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[9]])
ppc10 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[10]])
ppc11 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[11]])
ppc12 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[12]])
ppc13 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[13]])
ppc14 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[14]])
ppc15 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[15]])
ppc16 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[16]])
ppc17 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[17]])
ppc18 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[18]])

bayesplot_grid(ppc1, ppc2, ppc3, ppc4, subtitles = c("texture_mean", "area_mean", "smoothness_mean", "concavity_mean"))
bayesplot_grid(ppc5, ppc6, ppc7, ppc8, subtitles = c("symmetry_mean", "fractal_dimension_mean", "texture_se", "area_se"))
bayesplot_grid(ppc9, ppc10, ppc11, ppc12, subtitles = c("smoothness_se", "concavity_se", "symmetry_se", "fractal_dimension_se"))
bayesplot_grid(ppc13, ppc14, ppc15, ppc16, subtitles = c("smoothness_worst", "concavity_worst", "smoothness_worst", "concavity_worst"))
bayesplot_grid(ppc17, ppc18, subtitles = c("symmetry_worst", "fractal_dimension_worst"))
```

```{r}
p <- list()
for (i in 1:length(pareto_k_values)){
  df <- data.frame(pareto_k_values[[i]])
  colnames(df) <- c("values")
  p[[i]] <- ggplot(data=df, aes(y=values, x=1:nrow(cancer_data))) + 
    geom_point(color="#10A5F5") + 
    geom_hline(yintercept=0.7, linetype="dashed", size=1, color="#0859C6") + 
    ggtitle(variables[i]) +  xlab("Samples") + ylab("K-values")
}
do.call(grid.arrange, c(p[1:4], ncol=2))
do.call(grid.arrange, c(p[5:8], ncol=2))
do.call(grid.arrange, c(p[9:12], ncol=2))
do.call(grid.arrange, c(p[13:16], ncol=2))
do.call(grid.arrange, c(p[17:18], ncol=2))
```


# Convergence diagnostics
## $\hat{R}
$\hat{R}$ can be used to monitor if a group of chains has converged to the target distribution. This can be done by comparing the between and within-chain estimate for model parameters. If chains have not converged, meaning that they have not mixed well, $\hat{R}$ will be greater than one. If chains have converged, the $\hat{R}$ value will be virtually one.

As seen in the `Appendix A` `monitor` outputs, all $\hat{R}$ values were 1 or close to 1, which means that the Linear Model chains have converged.

## ESS
We would like to have a diagnostic that tell us when the weights are problematic, as we could have vastly imbalanced weights. In this case, we could use **effective sample size ($S_{eff}$)**, a quantitative measure of efficiency in **importance sampling**. $S_{eff}$ represents the number of independent samples required to obtain an importance sampling estimate with about the same efficiency as if we would have used all the samples.

As seen in `Appendix A`, the `monitor` function gives us two crude measures of effective sample size called Bulk_ESS and Tail_ESS (an $ESS > 100$ per chain is considered good). Additionally, our ESS values are way higher than 100, which established that our values are good.

## Divergences

A divergence arises when the simulated Hamiltonian trajectory departs from the true trajectory as measured by departure of the Hamiltonian value from its initial value [...]. Habrá que indicar si hemos obtenido divergencias o no en los modelos más adelante con la siguiente función:

```{r}
for (i in (1:length(linear_models))){
  cat("\n\n- Checking divergence for variable: ", variables[i], "\n")
  check_hmc_diagnostics(linear_models[[i]])
}
```

As seen in the obtained results, we obtained 0 divergences... etc

# Multivariate Model
```{r}

model_accuracies <- function(model){
  y_pred <- posterior_epred(model)
  preds <- colMeans(y_pred)
  prueba <- predict(model, newdata=scaled_cancer_data, type='response')
  print(length(prueba))
  # Class Negative Accuracy
  pr <- as.integer(preds >= 0.5)
  acc_zero <- round(mean(xor(pr, as.integer(scaled_cancer_data$diagnosis==0))), 4)
  
  # Class Positive Accuracy
  pr <- as.integer(preds < 0.5)
  acc_one <- round(mean(xor(pr, as.integer(scaled_cancer_data$diagnosis==1))), 4)
  print(length(y_pred))
  print(length(scaled_cancer_data$diagnosis))
  print(confusionMatrix(factor(round(prueba)),factor(scaled_cancer_data$diagnosis)))
  return (c(acc_zero, acc_one))
}

writeLines(readLines("models/linear_model_bernoulli_multivariate.stan"))

beta <- student_t(df = 1, location = 0, scale = 2)
alpha <- normal(0, 10)

linear_model_bernoulli_multivariate <- rstan::stan_model(file = "models/linear_model_bernoulli_multivariate.stan")
```

## Multivariate Model - w/ Mean Variables

```{r}

mv_mean_block <- list("texture_mean", "area_mean", "smoothness_mean",
                      "concavity_mean", "symmetry_mean", "fractal_dimension_mean")
mv_se_block <- list("texture_se", "area_se", "smoothness_se", "concavity_se",
                       "symmetry_se", "fractal_dimension_se")
mv_worst_block <- list("texture_worst", "area_worst", "smoothness_worst",
                       "concavity_worst", "symmetry_worst", "fractal_dimension_worst")
mv_var_block <- list(mv_mean_block, mv_se_block, mv_worst_block)

multivariate_models <- c()
multivariate_glms <- list()

for (i in (1:3)){
  var_block <- mv_var_block[[i]]
  print(paste("diagnosis ~", paste(var_block, collapse = "+")))
  multivariate_block_data <- list(N=length(scaled_cancer_data$diagnosis), 
                               y=c(scaled_cancer_data$diagnosis), 
                               x_1=scaled_cancer_data[,var_block[[1]]],
                               x_2=scaled_cancer_data[,var_block[[2]]],
                               x_3=scaled_cancer_data[,var_block[[3]]],
                               x_4=scaled_cancer_data[,var_block[[4]]],
                               x_5=scaled_cancer_data[,var_block[[5]]],
                               x_6=scaled_cancer_data[,var_block[[6]]])
  multivariate_sampling <- rstan::sampling(linear_model_bernoulli_multivariate,
                                           data = multivariate_block_data,
                                           seed = SEED, refresh=0)
  multivariate_models <- c(multivariate_models, multivariate_sampling)
  
  multivariate_glm <- stan_glm(paste("diagnosis ~", paste(var_block, collapse = "+")),
                               data = scaled_cancer_data,
                               family = binomial(link = "logit"), 
                               prior = beta, prior_intercept = alpha, QR=TRUE,
                               seed = SEED, refresh=0)
  multivariate_glms <- append(multivariate_glms, list(multivariate_glm))
}

```

## Multivariate Model - Diagnostics and Comparisons
Worst is best.
```{r}
pareto_k_values <- list()
multivariate_loo_estimates_df <- data.frame()
mv_model_names <- c("mean", "se", "worst")
#install.packages("caret")
library(caret)
library(e1071)
library(pROC)
for (i in (1:length(multivariate_models))){
  cat("- Monitoring variable group: ", mv_model_names[i], "\n")
  model <- multivariate_models[[i]]
  model_log_lik <- extract_log_lik(model, parameter_name = "log_lik", merge_chains = FALSE)
  model_r_eff <- relative_eff(exp(model_log_lik))
  model_loo <- loo(model_log_lik, r_eff = model_r_eff)
  pareto_k_values <- append(pareto_k_values, list(model_loo$diagnostics$pareto_k))
  
  accs <- model_accuracies(model = multivariate_glms[[i]])
  
  aux_df <- data.frame("variable group" = mv_model_names[i], 
                       "elpd_loo" = model_loo$estimates[1], 
                       "p_loo" = model_loo$estimates[2],
                       "pred. negative" = accs[1],
                       "pred. positive" = accs[2])
  
  multivariate_loo_estimates_df <- rbind(multivariate_loo_estimates_df, aux_df)
}
# Escribir sobre esto: https://boostedml.com/2019/05/classification-accuracy-in-r-difference-between-accuracy-precision-recall-sensitivity-and-specificity.html#Classification_Categories_and_the_Confusion_Matrix
cat("-- Linear Model Comparison")
print(multivariate_loo_estimates_df)
```

# Plot simulated data
```{r}
y_sims <- list()
for (model in multivariate_models){
  params <- extract(model)
  y_sims <- append(y_sims, list(params$y_sim[1:100,]))
}

ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[1]])
ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[2]])
ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[3]])
```


```{r}
multivariate_full <- stan_glm(diagnosis ~ texture_mean + area_mean + smoothness_mean + concavity_mean
                              + symmetry_mean + fractal_dimension_mean
                              + texture_se + area_se + smoothness_se + concavity_se
                              + symmetry_se + fractal_dimension_se
                              + texture_worst + area_worst + smoothness_worst + concavity_worst
                              + symmetry_worst + fractal_dimension_worst,
                              data = scaled_cancer_data,
                              family = binomial(link = "logit"), 
                              prior = beta, prior_intercept = alpha, QR=TRUE,
                              seed = SEED, refresh=0)
```



```{r}
accs <- model_accuracies(model = multivariate_full)

model_log_lik <- log_lik(multivariate_full)
model_loo <- loo(model_log_lik)
pareto_k_values <- append(pareto_k_values, list(model_loo$diagnostics$pareto_k))

aux_df <- data.frame("variable group" = "full model",
                     "elpd_loo" = model_loo$estimates[1],
                     "p_loo" = model_loo$estimates[2],
                     "pred. negative" = accs[1],
                     "pred. positive" = accs[2])
multivariate_loo_estimates_df <- rbind(multivariate_loo_estimates_df, aux_df)

yrep <- posterior_predict(multivariate_full, draws = 50)
ppc_dens_overlay(scaled_cancer_data$diagnosis, yrep)

mcmc_areas(as.matrix(multivariate_full))

print(multivariate_loo_estimates_df)
```

# EJECUTAR UNA UNICA VEZ PARA CANTIDAD DE VARIABLES POR USAR
```{r}
refmodel <- get_refmodel(multivariate_full)
vs <- cv_varsel(refmodel, method = "forward")
solution_terms(vs)
plot(vs, stats = c('elpd', 'rmse'))
```


```{r}
#area_worst, area_mean, area_se, concavity_mean.

multivariate_three <- stan_glm(diagnosis ~ area_worst + concavity_worst + texture_worstre_worst,
                               data = scaled_cancer_data,
                               family = binomial(link = "logit"), 
                               prior = beta, prior_intercept = alpha,
                               seed = SEED, refresh=0)

accs <- model_accuracies(model = multivariate_full)

model_log_lik <- log_lik(multivariate_three)
model_loo <- loo(model_log_lik)
pareto_k_values <- append(pareto_k_values, list(model_loo$diagnostics$pareto_k))

aux_df <- data.frame("variable group" = "three vars",
                     "elpd_loo" = model_loo$estimates[1],
                     "p_loo" = model_loo$estimates[2],
                     "pred. negative" = accs[1],
                     "pred. positive" = accs[2])
multivariate_loo_estimates_df <- rbind(multivariate_loo_estimates_df, aux_df)

yrep <- posterior_predict(multivariate_full, draws = 50)
ppc_dens_overlay(scaled_cancer_data$diagnosis, yrep)

yrep <- posterior_predict(multivariate_three, draws = 50)
ppc_dens_overlay(scaled_cancer_data$diagnosis, yrep)

mcmc_areas(as.matrix(multivariate_three))

print(multivariate_loo_estimates_df)
```


# Model Comparison
## Full multivariate model
```{r}
loo1 <- loo(post1, save_psis = TRUE)
loo1
```
## Multivariate model with 1 or more variables
```{r}
loo2 <- loo(post2, save_psis = TRUE)
loo2
```
## Model comparison
```{r}
loo_compare(loo1, loo2)
```










# Appendices
## Apendix A - Linear Model monitors

```{r}
for (model in linear_models){
  monitor(model)
}
```




