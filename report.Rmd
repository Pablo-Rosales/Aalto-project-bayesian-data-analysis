---
title: "Project BDA"
author: "Anonymous"
date: "11/14/2020"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '1'
    df_print: paged
header-includes: 
- \usepackage{float}
- \usepackage{subfig}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loaded packages

```{r, message=FALSE, warning=FALSE}
library(rstan) 
rstan_options(auto_write = TRUE)
options(mc.cores = 3)
library(ggplot2)
library(aaltobda)
library(shinystan)
library(bayesplot)
library(loo)
library(dlookr)
library(corrplot)
library(rstanarm)
library(projpred)
library(GGally)
library(shiny)
library(gridExtra)
library(caret)
library(e1071)
library(pROC)
SEED <- 48927
```


\newpage
# Introduction

Breast cancer most commonly presents as a lump that feels different from the rest of the breast tissue. More than 80% of cases are discovered when a person detects such a lump with the fingertips and there are various methods of assessing if the detected lump is a cyst, and if so, either benign or malignant.

One such method of detecting the dangerousness of the mass is Fine-needle Aspiration (FNA). This diagnostic procedure consists of a very safe and minor procedure, where a thin hollow needle is inserted into the mass for obtaining cell samples and analyzing them under a microscope. A major surgical biopsy can be avoided by performing FNA, which is safer and far less traumatic, and possibly eliminating the need for hospitalization and other complications.

The problem presented in this report deals with finding out which features of a FNA are more relevant in diagnosing a patient's mammary lump as benign or malignant. The data used is the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29), a data set whose features are computed from digitized images of FNAs of breast mass.

## Main Modeling Idea

As a summary, our modeling idea has been to do the following analysis' with different types of models:

1. **Linear Model**, with each of the remaining 18 features, to see which individual variable might have more influence in predicting correct diagnosis.
2. **Multivariate Models**, done with 3 blocks of 6 variables from mean, se and worst, to see which variable block might have more influence in predicting correct diagnosis.
3. **Multivariate Model**, done with all 18 features in order to obtain proper posterior checking, and see the minimum optimal amount of variables needed, and which ones, for equal or better prediction as this model with all 18 features.
4. **Multivariate Model**, done with the best minimum optimal amount of variables, to check if the predictions obtained are equal or better than the model with all variables.
5. **Multivariate Model**, done with Regularized Horseshoe Prior with the best minimum optimal amount of variables, in order to check if the predictions obtained are equal or better than the model with all variables.
6. **Gaussian Model**, done with the four best variables obtained from applying the Multivariate model in order to compare the results of applying it to the dataset and check if it performs better with the selected variables than the considered alternatives. 

# Exploratory Data Analysis

The breast cancer dataset consists of 569 FNA procedures each with 32 features. Ten real-valued features are computed for each cell nucleus:

a) radius
b) texture
c) perimeter
d) area
e) smoothness
f) compactness
g) concavity
h) concave points
i) symmetry
j) fractal dimension

From these, the mean, standard error and worst, are computed, thus making 10 features into 30. The other 2 features are the FNA ID number and the diagnosis, the target binary variable, which has values 'M' (malignant) or 'B' (benign).

```{r}
cancer_data = read.csv('cancer.csv')
head(cancer_data)
```

```{r}
nrows <- nrow(cancer_data)
ncols <- ncol(cancer_data)
cat("Breast cancer dataset size:", nrows, "rows x", ncols, "cols")
```

As an addendum when talking about the amount of features the dataset has, it can be noticed that instead of the 32 features mentioned, there are really 33 columns, this is because the last column, called 'X', is a pointless column present filled with 'N/A' values. This column will be dropped.

```{r}
cancer_data$X <- NULL
cat("Dataset contains NULL values:", any(is.na(cancer_data)), '\n')
```

In order to present our data into an numeric format, we convert the target output into binary values (B into '0' and M into '1')

```{r}
cancer_data[cancer_data == "B"] <- as.numeric(0)
cancer_data[cancer_data == "M"] <- as.numeric(1)
cancer_data$diagnosis <- as.numeric(as.character(cancer_data$diagnosis))
```

Is our data balanced? We would like to know more about how many benign and malignant tumors are contained in our dataset by visualizing a barplot.

```{r, fig.height=3, fig.width=4}
ggplot(cancer_data, aes(factor(diagnosis), fill=as.factor(diagnosis))) +
      geom_bar() + theme(legend.position="none") + xlab("Diagnosis") +
      ggtitle("Benign and malignant tumor")
```

A correlation matrix is visualized to obtain correlation coefficients between variables.

```{r}
corrplot(cor(cancer_data[,2:32]))
```

As seen in the correlation matrix, there are some variables that are *highly* correlated. Since they do not provide any new information, we could apply feature selection. For instance, `radius_mean`, `perimeter_mean` and `area_mean` are highly correlated, so we decide to keep `area_mean` in our dataset. Moreover, `compactness_mean`, `concavity_mean` and `concave.points_mean` are correlated, so we decide to keep `concavity_mean`. This process is also carried out for the standard deviation and worst related variables.

```{r}
# Dropping columns using the subset function. '-' indicates dropping vars. 
cancer_data = subset(cancer_data, select = -c(radius_mean, perimeter_mean, 
                     compactness_mean, concave.points_mean, radius_se, 
                     perimeter_se, compactness_se, concave.points_se, 
                     radius_worst, perimeter_worst, compactness_worst, 
                     concave.points_worst))

variables <- c("texture_mean", "area_mean", "smoothness_mean", "concavity_mean",
               "symmetry_mean", "fractal_dimension_mean", "texture_se",
               "area_se", "smoothness_se", "concavity_se", "symmetry_se",
               "fractal_dimension_se", "texture_worst", "area_worst", 
               "smoothness_worst", "concavity_worst", "symmetry_worst", 
               "fractal_dimension_worst")
```

By performing this feature selection, we drastically removed the number of variables, from 30 to 18. The final variables, along with the correlation, used for our models are as follows:

```{r}
corrplot(cor(cancer_data))
```

There are still some variables that are *slightly* correlated, but a more curated feature selection will be made later on in order to create our models.

Now, we would like to **scale** the columns of our data into the range [0,1] for easier comparison, except the diagnosis and id column. Thus, each column has a mean of 0 with a standard deviation of 1.

```{r}
scaled_cancer_data <- cancer_data
col_names <- colnames(cancer_data) # Keep column names

scaled_cancer_data[,1:2] <- cancer_data[,1:2] # Id and diagnosis without scaling
scaled_cancer_data[,3:18] <- scale(cancer_data[,3:18])
colnames(scaled_cancer_data) <- col_names # Retrieve column names
head(scaled_cancer_data)
```

# Priors

We have chosen to work with Weakly Informative Priors because:

1. A weakly informative prior means a reasonable representation of partial ignorance about the data, but that still includes a small amount of real-world information. Uniformity of distribution on an appropriate measurement scale means that the prior does not strongly favor particular values of the parameter.

2. A weakly informative prior does not contribute strongly to the posterior. With a weakly informative prior we say that this small contribution from the prior "lets the data speak for itself".

Because the target feature, the diagnosis, has binary values we will use a Bernoulli logistic regression approach: 

$$ y \sim Bernoulli(y\ |\ logit^{-1}(\alpha + \beta \times x)),$$

with $\alpha$ and $\beta$ are the intercept and regression coefficients, $x$ are the predictor variables and $y$ is the breast cancer diagnosis.

## Coefficients

For the $\alpha$ intercept coefficient and $\beta$ regression coefficient we have chosen a Normal and a Student T distribution respectively, to be more precise:

1. $\alpha \sim Normal(\mu_{\alpha},\ \sigma_{\alpha})$, with $\mu_{\alpha} = 0$ and $\sigma_{\alpha} = 1$.
2. $\beta \sim StudentT(df_{\beta},\ location_{\beta},\ scale_{\beta})$, with $df_{\beta} = 3$, $location_{\beta} = 0$ and $scale_{\beta} = 1$.

The reason behind this choice in priors for the intercept coefficient $\alpha$ is because we scale our data, so it makes sense to select a normal distribution with mean centered in 0 and standard deviation 1. On the other hand, for the regression coefficient $\beta$ we chose a StudenT distribution with 3 degrees of freedom and scale 1 because we want to ensure a finite variance and also a finite mean, which will create thick tails that reflect our uncertainty in the prior scale.

```{r}
beta <- student_t(df = 3, location = 0, scale = 1)
alpha <- normal(0, 1)
```

## Regularized Horseshoe Prior

We have also used a Weakly Regularized Horseshoe Prior, as *Piironen+Vehtari:RHS:2017*, where we included the prior assumption that a number of variables are more relevant that the rest. The assumption origin will be discussed in a future section, but as a summary, it has been assumed that there are 3 relevant variables, while the others are irrelevant.

$$
\begin{aligned}
\beta_m &\sim N(0,\tau \times \tilde{\lambda}) \\[2pt]
\tilde\lambda_m &\sim \frac{c \times \tilde\lambda_m}{\sqrt{c^2 + \tau^2\times \tilde\lambda_m^2}} \\[2pt]
\lambda_m &\sim HalfCauchy(0,1) \\[2pt]
c^2 &\sim InvGamma(\frac{v}{2},\frac{v}{2}\times s^2)\\[2pt]
\tau &\sim HalfCauchy(0,\tau_0) \\[2pt]
\end{aligned}
$$

```{r}
p_0 <- 3 # number of relevant variables
tau_0 <- p_0 / (length(variables) - p_0) * 1 / sqrt(nrows)
rhs_prior <- hs(global_scale = tau_0)
```

## Gaussian Model

Gaussian processes are parameterized by a mean vector and a covariance matrix. The chosen kernel for implementing the covariance is the exponentiated quadratic kernel, which depends on two hyperpriors, the length scale, $\rho$, and the marginal standard deviation $\alpha$. The latter measures the average distance from the mean function, while the former represents the frequency of the function.

$$
k(x|\alpha, \rho)_{ij} = \alpha^2 exp(\frac{-1}{2\rho^2}\sum_{d=1}^D(x_{i,d}-x_{j,d})^2)
$$

Hence, the priors and hyperpriors are as following:

$$
\begin{aligned}
\rho &\sim InvGamma(1,1) \\[2pt]
\alpha &\sim Normal(0,1) \\[2pt]
a &\sim Normal(0,1) \\[2pt]
\eta &\sim Normal(0,1) \\[2pt]
\end{aligned}
$$

Where the vector eta is added to increase the efficiency of the sampling.

# Models
Throughout this section, the designed modeling ideas will be presented in its respective subsection: explanation of the model and code.

There are specific sections after this one for the evaluations, comparisons and diagnostics of the models. These will not be mentioned in the ´Models´ section.

## Linear Models
Per each variable contained in the data set, a separate linear model is created in order to observe its efficiency and relevance.

### Stan code

The common **stan code** for each variable is designed as follows:

```{r}
writeLines(readLines("models/linear_model_bernoulli.stan"))
linear_model <- rstan::stan_model(file = "models/linear_model_bernoulli.stan")
```
As seen in the stan model above, the priors are $\alpha \sim N(0,1)$ for the intercept coefficient and $\beta \sim StudentT(3,0,1)$ for the regression coefficient. The likelihood has been calculated using `bernoulli_logit` for the binary outcome. In order to compute the log-likelihood of the model `bernoulli_logit_lpmf` has been used as well as `bernoulli_logit_rng` to compute the predictions of the outcome.

### Sampling

The following code draws samples from the defined stan model. Each iteration pertains to a variable of the data set ('texture_mean', 'area_mean', etc...) contained in `variables` variable. Additionally to each of the variables, the sampling also receives as input the number `N` of observations of the dataset and the binary target value (diagnosis) for each of the observations.

```{r}
linear_models <- c()
y <- c(scaled_cancer_data$diagnosis) # Binary outcomes
for (variable in variables){
  cat("Sampling with variable: ", variable, "\n")
  # Data
  linear_model_data <- list(N=nrows, y=y, x=c(scaled_cancer_data[,variable]))

  # Model sampling
  linear_sampling <- rstan::sampling(linear_model, data = linear_model_data,
                                     seed = SEED, refresh=0)

  # Model saved into a list for further analysis later on.
  linear_models <- c(linear_models, linear_sampling)
}
```

## Multivariate Model - Variable Blocks

Followed by the analysis done for each of the variables, it was decided to create three separate multivariate models, one for each variable block (type of variable): **mean**, **se** and **worst**. This was done in order to observe the efficiency and relevance over the type of the variable and see if some of the measure blocks shouldn't be considered when making the diagnosis.

### Sampling

The following code draws samples from the defined stan model. Each iteration pertains to a variable block ('mean', 'se' and 'worst'), comprised of 6 variables for each of them ('texture', 'area', 'smoothness', 'concavity', 'symmetry', 'fractal_dimension'), which are passed as predictors to the model. Additionally to each of the variable blocks, the sampling also receives as input the number `N` of observations of the dataset and the binary target value (diagnosis) for each of the observations.

Additionally to the `rstan::sampling` models obtained, the same models were fitted with `stan_glm`, in order to be able to make use of functions that need the input to be a stan_glm object. Both types of model objects have been saved for posteriority, for obtaining the needed diagnostics.

```{r}
mv_mean_block <- list("texture_mean", "area_mean", "smoothness_mean",
                      "concavity_mean", "symmetry_mean", 
                      "fractal_dimension_mean")

mv_se_block <- list("texture_se", "area_se", "smoothness_se", "concavity_se",
                       "symmetry_se", "fractal_dimension_se")

mv_worst_block <- list("texture_worst", "area_worst", "smoothness_worst",
                       "concavity_worst", "symmetry_worst", 
                       "fractal_dimension_worst")

mv_model_names <- c("Mean Model", "SE Model", "Worst Model", 
                    "Full Variables Model", "Three variables Model", 
                    "RHS Model")

mv_var_block <- list(mv_mean_block, mv_se_block, mv_worst_block)

# multivariate_models <- c()
multivariate_glms <- list()

for (i in (1:3)){
  var_block <- mv_var_block[[i]]
  print(paste("diagnosis ~", paste(var_block, collapse = "+")))
  multivariate_block_data <- list(N=length(scaled_cancer_data$diagnosis), 
                               y=c(scaled_cancer_data$diagnosis), 
                               x_1=scaled_cancer_data[,var_block[[1]]],
                               x_2=scaled_cancer_data[,var_block[[2]]],
                               x_3=scaled_cancer_data[,var_block[[3]]],
                               x_4=scaled_cancer_data[,var_block[[4]]],
                               x_5=scaled_cancer_data[,var_block[[5]]],
                               x_6=scaled_cancer_data[,var_block[[6]]])
  
  # multivariate_sampling <- rstan::sampling(linear_model_bernoulli_multivariate,
  #                                          data = multivariate_block_data,
  #                                          seed = SEED, refresh=0)
  # 
  # multivariate_models <- c(multivariate_models, multivariate_sampling)
  
  multivariate_glm <- stan_glm(paste("diagnosis ~",
                                     paste(var_block, collapse = "+")),
                               data = scaled_cancer_data,
                               family = binomial(link = "logit"), 
                               prior = beta, prior_intercept = alpha, QR=TRUE,
                               seed = SEED, refresh=0)
  multivariate_glms <- append(multivariate_glms, list(multivariate_glm))
}
```

### Stan Code

The priors are $\alpha \sim N(0,1)$ for the intercept coefficient and $\beta_n \sim StudentT(3,0,1)$ for each of the regression coefficients.

For convenience we ended up sampling with `stan_glm`, whose code can be found in the [Appendix D - Model Code](#apb) in the `Multivariate Variable Block Stan_GLM Code`, because there are many models and we needed to use functions that required `stan_glm` objects as input. 

For this reason, even though we originally started with `.stan` code, the sampling with it has been omitted in the sampling code above. Finally the `.stan` code can be found in the [Appendix D - Model Code](#apb) in the `Multivariate Variable Block Stan Code` if one wishes to see it. Originally, the likelihood has been calculated using `bernoulli_logit` for the binary outcome. In order to compute the log-likelihood of the model `bernoulli_logit_lpmf` has been used as well as `bernoulli_logit_rng` to compute the predictions of the outcome.

## Multivariate Model - Full Model

Additionally, we decided to carry out a more sensible approach, which was to start by creating and fitting a model which contains all of the 18 variables.

The reasoning behind this model is to carry out projection predictive variable by analyzing the coefficient marginal posteriors and the solution terms, in order to obtain possible information regarding the optimal number of variables that should be used for the data and which of the variables (and in which order) have more weight in the data.

### Sampling and Stan Code

For this model, we used `generalized linear model`, `stan_glm`, using the same prior distributions for the intercept and regression coefficients as the previous models, this is: $\alpha \sim N(0,1)$ for the intercept coefficient and $\beta_n \sim StudentT(3,0,1)$ for the regression coefficient.

In order for the model to compute the predictions of the diagnosis outcome we used binomial logistic regression, by making use of `stan_glm`'s `family` for which we chose `binomial` with `link = "logit"`. The `.stan` code can be found in the [Appendix D - Model Code](#apb) in the `Multivariate Full Model Stan_GLM Code`.

The formula includes the predictive and target variables.

```{r, warning=FALSE}
multivariate_full <- stan_glm(diagnosis ~ texture_mean + area_mean
                              + smoothness_mean + concavity_mean
                              + symmetry_mean + fractal_dimension_mean
                              + texture_se + area_se + smoothness_se
                              + concavity_se + symmetry_se
                              + fractal_dimension_se + texture_worst
                              + area_worst + smoothness_worst + concavity_worst
                              + symmetry_worst + fractal_dimension_worst,
                              data = scaled_cancer_data,
                              family = binomial(link = "logit"), 
                              prior = beta, prior_intercept = alpha, QR=TRUE, 
                              adapt_delta=0.9999, iter=5000,
                              seed=SEED, refresh=0)
multivariate_glms <- append(multivariate_glms, list(multivariate_full))
```

### Coefficient Marginal posteriors

For projection predictive variable selection we firstly analyzed the coefficient marginal posteriors by obtaining the plot below making use of `mcmc_areas`.

We can see that the posterior coefficient for a few variables is far away from zero or have wide margins, but it's not clear what variables should be included, or how many. Some, such as `texture_mean` and `smoothness_mean`, are hinted as potentially relevant variables with correlation in joint posterior.

Just looking at the marginals has the problem that it can perfectly occur that correlating variables have marginal posteriors overlapping zero or far from zero, but joint posterior typically does not include zero. Because of this, further projection predictive variable selection must be carried out.

```{r}
mcmc_areas(as.matrix(multivariate_full))
```

### Solution Terms

Because we want to subject all variables to selection, we continue with projection predictive variable selection which can be easily done with `cv_varsel` function. This also computes a LOO-CV estimate of the predictive performance for the best models with a certain number of variables.

As mentioned above, one of the reasonings behind using this full model is to obtain the optimal number of variables for which the predictive performance is the same as the full model or better. By looking at the plot we can see that we can achieve a higher predictive performance using only 3 variables instead of 18. This can be corroborated further by running the `suggest_size` function, which tells us that the optimal number of variables for equal performance is 2, but better results can be achieved with 3.

```{r, echo=FALSE, fig.cap='cv varsel function output', fig.align='center', fig.pos="H", out.width = '80%'}
knitr::include_graphics('images/cv_varsel.png')
```

The other reasoning behind the full model was which of the variables are the best for obtaining better predictive performance. We can obtain a ranking of best variables to be inserted in the model with `solution_terms`, and from this ranking we choose the first 3 best variables: `area_worst`, `smoothness_mean` and `texture_mean`.

```{r, echo=FALSE, fig.cap='solution terms function output', fig.align='center', fig.pos="H", out.width = '80%'}
knitr::include_graphics('images/solution_terms.png')
```

Note: these functions are computationally complex and require quite a large amount of time to be executed and we needed to run the code many times to fix errors and other reasons. Because of this, the code block has been made to pass the execution, but photos of the correct execution have been inserted.

```{r, eval=FALSE, echo=FALSE}
refmodel <- get_refmodel(multivariate_full)
vs <- cv_varsel(refmodel, method = "forward", cv_method = 'LOO', nloo = nrows)
suggest_size(vs, alpha=0.1)
solution_terms(vs)
plot(vs, stats = c('elpd', 'rmse'))
```

## Multivariate Model - Three Variables Model

This model is the result of applying projection predictive variable selection to the full multivariate model of 18 variables. In this model we simply take the 3 best variables obtained using the previous analysis and study the outcomes.

### Sampling and Stan Code

For this model, we used `generalized linear model`, `stan_glm`, using the same prior distributions for the intercept and regression coefficients as the previous models, this is: $\alpha \sim N(0,1)$ for the intercept coefficient and $\beta_n \sim StudentT(3,0,1)$ for the regression coefficient.

In order for the model to compute the predictions of the diagnosis outcome we used binomial logistic regression, by making use of `stan_glm`'s `family` for which we chose `binomial` with `link = "logit"`. The `.stan` code can be found in the [Appendix D - Model Code](#apb) in the `Multivariate Three Variables Model Stan_GLM Code`.

The formula includes the 3 best ranked predictive variables and the target.

```{r}
multivariate_three <- stan_glm(diagnosis ~ area_worst + smoothness_mean
                               + texture_mean, data = scaled_cancer_data,
                               family = binomial(link = "logit"), 
                               prior = beta, prior_intercept = alpha,
                               seed = SEED, refresh=0)
multivariate_glms <- append(multivariate_glms, list(multivariate_three))
```

## Multivariate Model - Regularized Horseshoe Model

As we already know that most of the variables are irrelevant thanks to the previous projection predictive variable selection analysis, and only three variables are relevant in order to obtain a better performance, we decided to implement a Regularized Horseshoe Prior approach which has been proven to be a worthy alternative.

### Sampling and Stan Code

For this model, we used `generalized linear model`, `stan_glm`, but instead of using the same priors for the intercept and regression coefficients we use the weakly informative regularized horseshoe prior which includes prior assumption that some of the variables might be irrelevant.

This is the reason why we choose $p_0 = 3$, and we pass this to the regularized horseshoe prior, which will in turn use it to calculate the respective distributions.

In order for the model to compute the predictions of the diagnosis outcome we used binomial logistic regression, by making use of `stan_glm`'s `family` for which we chose `binomial` with `link = "logit"`. The `.stan` code can be found in the [Appendix D - Model Code](#apb) in the `Multivariate RHS Model Stan_GLM Code`.

The formula for the regularized horseshoe prior is the same as the multivariate model with all variables, it includes all of the 18 predictive variables and the target.

```{r}
multivariate_rhsp <- stan_glm(diagnosis ~ texture_mean + area_mean
                              + smoothness_mean + concavity_mean
                              + symmetry_mean + fractal_dimension_mean
                              + texture_se + area_se + smoothness_se
                              + concavity_se + symmetry_se
                              + fractal_dimension_se + texture_worst
                              + area_worst + smoothness_worst + concavity_worst
                              + symmetry_worst + fractal_dimension_worst,
                              data = scaled_cancer_data,
                              family = binomial(link = "logit"),
                              prior=rhs_prior, QR=TRUE,
                              seed = SEED, refresh=0)
multivariate_glms <- append(multivariate_glms, list(multivariate_rhsp))
```

## Gaussian Model
The last model that was considered for this project was a Gaussian Process based model. Gaussian models provide a probability distribution over functions. They are useful for representing those datasets that cannot be easily adjusted to a line following a regression procedure. A gaussian process consists of generating higher degree polynomial functions and assigning to those functions a probability. Therefore, by taking the mean over the probability distribution, the most probable fit is obtained.

### Stan code

The common **stan code** for each variable is designed as follows:

```{r}
writeLines(readLines("models/gaussian_process_bernoulli.stan"))
``` 

In order to later assess the performance of the gaussian model based on the quality of its predictions, the data set is divided into two subsets, one for "training" the model, containing a higher number of observations, and another one for "testing" the model.

```{r, eval=FALSE}
train_index <- sample(1:nrow(scaled_cancer_data), 0.8*nrow(scaled_cancer_data))
test_index <- setdiff(1:nrow(scaled_cancer_data), train_index)

train_set <-scaled_cancer_data[train_index, ]
test_set <- scaled_cancer_data[test_index, ]
```

The gaussian model was carried out over the three best variables contained in the set of optimal variables obtained using the previous models.

```{r, eval=FALSE}
variables_gaussian <- c("area_worst", "smoothness_mean", "texture_mean")
gaussian_fits <- c()
x_predicts_idx <- c()
N_obs <- 200
N_predict <- 100
gaussian_model <- rstan::stan_model(file = 
                                      "models/gaussian_process_bernoulli.stan")
```

From the training set, 200 randomly selected observations are considered. Gaussian models are computationally slow, hence, using the total number of observations would take much time. From the test set, 100 observations are chosen, also randomly.

```{r, eval=FALSE}
for (variable in variables_gaussian){
  cat("\nSampling from model with variable: ", variable, "\n")
  x_obs_idx <- sort(sample(1:length(train_set[,variable]), N_obs))
  x_predict_idx <- sort(sample(1:length(test_set[,variable]), N_predict))
  x_predicts_idx <- c(x_predicts_idx, x_predict_idx)
  gaussian_model_data <- list(N_predict = N_predict,
                              x_predict = test_set[,variable][x_predict_idx],
                              N_obs = N_obs,
                              x_obs = train_set[,variable][x_obs_idx],
                              y_obs = train_set[,"diagnosis"][x_obs_idx])
  
  gaussian_fit <-rstan::sampling(gaussian_model, data = gaussian_model_data,
                                 iter=1000, chains=2, refresh=0)
  gaussian_fits <- c(gaussian_fits, gaussian_fit)
}
```

```{r, include=FALSE}
N_obs <- 200
N_predict <- 100
variables_gaussian <- c("area_worst", "smoothness_mean", "texture_mean")
x_predicts_idx <- readRDS("gaussian_models/x_predicts_idx.rds")
gaussian_fits <- c()

test_set <- readRDS("gaussian_datasets/test_set.Rda") 
train_set <- readRDS("gaussian_datasets/train_set.Rda")

for (i in 1:(length(variables_gaussian))){
  fit <- readRDS(paste("gaussian_models/",
                       variables_gaussian[i],"_gaussian_model.rds"))
  gaussian_fits <- c(gaussian_fits, fit)
}
```



# Convergence Diagnostics
## Diagnostic Metrics
The aim of this section is to assess the convergence of our models using a variety of diagnostics:

$\mathbf{\hat{R}}$ is a quantitative measure of convergence that will be used to monitor if a group of chains has converged to the target distribution. This can be done by comparing the between and within-chain estimate for model parameters. If chains have not converged, meaning that they have not mixed well, $\hat{R}$ will be greater than one. If chains have converged, the $\hat{R}$ value will be virtually one.

$\mathbf{Effective Sample Size (S_{eff})}$ is a quantitative measure of efficiency that will be used to check when the weights are problematic, as we could have vastly imbalanced weights. It represents the number of independent samples required to obtain an importance sampling estimate with about the same efficiency as if we would have used all the samples. The `monitor` function gives us two crude measures of effective sample size called Bulk_ESS and Tail_ESS (an $ESS > 100$ per chain is considered good).

$\mathbf{Divergence}$ checking will be used to check if a divergence arises when the simulated Hamiltonian trajectory departs from the true trajectory as measured by departure of the Hamiltonian value from its initial value. To evaluate this, we will use the function `check_hmc_diagnostics`.

## Linear Model Diagnostics
The diagnostics for the linear models can be seen in the [Appendix A - Linear models](#apa) `Monitor` and `Divergences` outputs.

All $\hat{R}$ values in the linear models were 1 or close to 1, which means that the chains have converged.

Additionally, the ESS values are way higher than 100, which establishes that our values are good.

Finally, we have obtained 0 divergences for all of our linear models.

## Multivariate Model Diagnostics
The diagnostics for the multivariate models can be seen in the [Appendix B - Multivariate models](#apb) `Monitor` and `Divergences` outputs.

All $\hat{R}$ values in the multivariate models were 1 or close to 1, which means that the chains have converged.

Additionally, the ESS values are way higher than 100, which establishes that our values are good.

All multivariate models turned great regarding divergences. It must be noted that the full multivariate model was problematic regarding this issue. At first, the model contained more than 300 divergences. To solve this, we tried different values of adapt_delta (0.95, 0.999) and increased the number of iterations up to 5000. We also tried increasing the number of chains but this wasn't a viable option as the computational time increased to an unrealistic point. Although the performance has improved considerably, some divergences still exist, which may be caused by the model's chains not visiting parts of the variable space with significant posterior mass leading to biased estimates. It must be noted that the number of divergences should be negligible as the full multivariate model only has 11 divergences out of 10000 iterations (0.11%).

## Gaussian Model Diagnostics
In order to visually assess the convergence of the gaussian model, the chains for the scalar parameters are plotted in the [Appendix C - Gaussian model](#apc). Also in the mentioned appendix, the values obtained from applying the function `monitor` are depicted. $\hat{R}$ provides a valid scalar metric for convergence checking based on computing the division between the pooled variance of the chains and the individual variance of each of them. Obtaining values close to 1, as in the current case, means that the chains happen to have very similar distributions. Finally, the results yielded for the effective sample size are higher than 100.

# Posterior Predictive Checking
## Linear Models

Kernel density estimate for the data and posterior predictive replicates of the linear models are slightly similar. From this we noticed that some variables might be worst for predictions, such as all of the `se` variables (``texture_se`, `symmetry_se`, etc.).

```{r}
y_sims <- list()
for (model in linear_models){
  params <- extract(model)
  y_sims <- append(y_sims, list(params$y_sim[1:100,]))
}

ppc1 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[1]])
ppc2 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[2]])
ppc3 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[3]])
ppc4 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[4]])
ppc5 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[5]])
ppc6 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[6]])
ppc7 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[7]])
ppc8 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[8]])
ppc9 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[9]])
ppc10 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[10]])
ppc11 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[11]])
ppc12 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[12]])
ppc13 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[13]])
ppc14 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[14]])
ppc15 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[15]])
ppc16 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[16]])
ppc17 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[17]])
ppc18 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[18]])

bayesplot_grid(ppc1, ppc2, ppc3, ppc4, ppc5, ppc6,
               subtitles = c("texture_mean", "area_mean",
                             "smoothness_mean", "concavity_mean",
                             "symmetry_mean", "fractal_dimension_mean"))
bayesplot_grid(ppc7, ppc8, ppc9, ppc10, ppc11, ppc12,
               subtitles = c("texture_se", "area_se",
                             "smoothness_se", "concavity_se",
                             "symmetry_se", "fractal_dimension_se"))
bayesplot_grid(ppc13, ppc14, ppc15, ppc16, ppc17, ppc18,
               subtitles = c("texture_worst", "area_worst",
                             "smoothness_worst", "concavity_worst", 
                             "symmetry_worst", "fractal_dimension_worst"))
```

## Multivariate Models

In the posterior predictive checking for each of the variable blocks, it is noticeable how much less the `se` block can influence the fitting of the model and thus getting better predictive results with these variables might not be a viable option. This is corroborated by the linear model predictive checking.

The `full`, `three variables` and `rhs` multivariate models have relatively similar kernel density estimate for the data.

```{r}
y_sims <- list()
for (model in multivariate_glms){
  params <- posterior_predict(model, draws = 100)
  head(params)
  y_sims <- append(y_sims, list(params))
}

ppc_mv_1 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[1]])
ppc_mv_2 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[2]])
ppc_mv_3 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[3]])
ppc_mv_4 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[4]])
ppc_mv_5 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[5]])
ppc_mv_6 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[6]])

bayesplot_grid(ppc_mv_1, ppc_mv_2, ppc_mv_3, ppc_mv_4, ppc_mv_5, ppc_mv_6,
               subtitles = c("Mean Model", "SE Model",
                             "Worst Model", "Full Model",
                             "Three Variables Model", "RHS Model"))
```

## Gaussian Model

The density of the diagnosis in the test set is printed in black color. Overlapping, the densities for the predictions of the first 50 iterations. This graph enables visual assessment of the quality of the predictions. Preliminarly, the results seem to be satisfactory.

```{r}
for (j in 1:(length(gaussian_fits))){
  params_gaussian <- extract(gaussian_fits[[j]])
  plot(density(test_set$diagnosis), xlim=c(-0.5, 1.5), ylim=c(0,2.5),
       main=variables_gaussian[j], ylab="density", xlab="", lwd=3)
  for (i in 1:50){
  lines(density(params_gaussian$y_predict[i,]), col="cyan")
  }
}
```

Another way to check if the predicted samples represent the reality is by directly plotting them (orange) and comparing with the target ones (grey). This is possible because the gaussian model is applied on values that are contained in the test set for the chosen variable. The concentration of points at 0 or 1 *y* values should be similar for the same values of *x*. In the given cases, by looking at the graphs, there is a very similar tendency between the real values and the simulated ones, meaning that the result is good.

```{r}
for (j in 1:(length(gaussian_fits))){
  params_gaussian <- extract(gaussian_fits[[j]])
  layout(mat = matrix(c(1,2), nrow = 1, ncol = 2),heights = c(1, 1), 
         widths = c(1,1))
  plot(c(range(test_set[,variables_gaussian[j]]
               [x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]])), c(-1,1.5),
       ty="n", main = variables_gaussian[j], xlab = variables_gaussian[j], 
       ylab = "diagnosis")
  points(test_set[,variables_gaussian[j]]
         [x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]], 
         test_set[,"diagnosis"]
         [x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]],
         col="grey", cex=1, pch=19)

  plot(c(range(test_set[,variables_gaussian[j]]
               [x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]]))
       , c(-1,1.5), ty="n", main = variables_gaussian[j], 
       xlab = variables_gaussian[j], ylab = "predicted diagnosis")
  for (i in 1:3){
    points(test_set[,variables_gaussian[j]]
           [x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]],
           params_gaussian$y_predict[i, 1:N_predict], col="orange",
           pch=3, cex=1)
  }
}
```

# Model comparisons
For model comparison we compare the models with **Leave-One-Out Cross Validation** (PSIS-LOO) and we assess the $\hat{k}$-values reliability for each of the models. If for the latter for the $\hat{k}$-values of a model are equal or lower than 0.7 we can consider that the model is reliable.

## Linear Models
We obtain the PSIS-LOO estimates for each of the linear models as well as their respective $\hat{k}$-values.

By looking at the `elpd_loo` and `p_loo` values for each of the models, we can deduce that the most reliable variables to be used are as follows (in order of best): `area_worst`, `area_mean`, `area_se`, `concavity_mean`.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Variable}   & \textbf{elpd\_loo} & \textbf{p\_loo} \\ \hline
14 - area\_worst    & -117.29            & 1.89            \\ \hline
2 - area\_mean      & -164.88            & 2.00            \\ \hline
8 - area\_se        & -181.79            & 2.20            \\ \hline
4 - concavity\_mean & -194.48            & 3.58            \\ \hline
\end{tabular}
\caption{Most reliable linear models.}
\end{table}

```{r}
pareto_k_values <- list()
linear_loo_estimates_df <- data.frame()

for (i in (1:length(linear_models))){
  cat("- Monitoring variable: ", variables[i], "\n")
  model <- linear_models[[i]]
  model_log_lik <- extract_log_lik(model, parameter_name = "log_lik",
                                   merge_chains = FALSE)
  model_r_eff <- relative_eff(exp(model_log_lik))
  model_loo <- loo(model_log_lik, r_eff = model_r_eff)
  pareto_k_values <- append(pareto_k_values,
                            list(model_loo$diagnostics$pareto_k))
  
  y_pred <- as.vector(extract(linear_models[[i]], pars="y_sim"))
  preds <- colMeans(y_pred$y_sim)
  pr <- as.integer(preds >= 0.5)
  acc <- round(mean(xor(pr, as.integer(scaled_cancer_data$diagnosis==0))), 4)

  aux_df <- data.frame("variable" = variables[i],
                       "elpd_loo" = model_loo$estimates[1],
                       "p_loo" = model_loo$estimates[2],
                       "accuracy" = acc)

  linear_loo_estimates_df <- rbind(linear_loo_estimates_df, aux_df)
}

print(linear_loo_estimates_df)
```

Most of the models have exceptional $\hat{k}$-values (lower than 0.7), which means that the results obtained by the models are reliable. The following output visualizes the Pareto-k values for each linear model.

```{r}
p <- list()
for (i in 1:length(pareto_k_values)){
  df <- data.frame(pareto_k_values[[i]])
  colnames(df) <- c("values")
  p[[i]] <- ggplot(data=df, aes(y=values, x=1:nrow(cancer_data))) +
    geom_point(color="#10A5F5") +
    geom_hline(yintercept=0.7, linetype="dashed", size=1, color="#0859C6") +
    ggtitle(variables[i]) +  xlab("Samples") + ylab("K-values")
}
do.call(grid.arrange, c(p[1:6], ncol=3))
do.call(grid.arrange, c(p[7:12], ncol=3))
do.call(grid.arrange, c(p[13:18], ncol=3))
```

## Multivariate Models
We obtain the PSIS-LOO estimates for each of the multivariate models as well as their respective $\hat{k}$-values.

What we take out of the analysis by variable blocks is that, in general, the `se` variables might not be as influential for predictions as the other blocks. Although this analysis did not bring a more specific result, it was an interesting experiment to carry out in order to check if any of the variable blocks might influence the final model too badly.

For the other models: `Full Variables Model`, `Three Variables Model` and `RHS Model`, the results are as we expected from before starting the analysis. The results for the `Full Variables Model` show that it isn't viable to use all of the 18 variables, because we obtain even better results by using the optimal number of variables and the specific variables we obtained from the projection predictive variable selection analysis, as can be seen in the results of `Three Variables Model` and `RHS Model`.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{variable.group} & \textbf{elpd\_loo} & \textbf{p\_loo} \\ \hline
1 Mean Model            & -83.85             & 5.68            \\ \hline
2 SE Model              & -173.03            & 12.22           \\ \hline
3 Worst Model           & -57.17             & 2.76            \\ \hline
4 Full Variables Model  & -95.77             & 5.78            \\ \hline
5 Three variables Model & -63.92             & 3.34            \\ \hline
6 RHS Model             & -48.70             & 14.45           \\ \hline
\end{tabular}
\end{table}

```{r}
# Auxiliary function to calculate the accuracy of a model
model_accuracies <- function(model){
  y_pred <- posterior_epred(model)
  preds <- colMeans(y_pred)
  pr <- as.integer(preds >= 0.5)
  acc <- round(mean(xor(pr, as.integer(scaled_cancer_data$diagnosis==0))), 4)
  
  conf_matrix <- confusionMatrix(factor(round(pr)), 
                                 factor(scaled_cancer_data$diagnosis))
  return (list(acc, list(conf_matrix)))
}
```

```{r, warning=FALSE}
pareto_k_values <- list()
multivariate_loo_estimates_df <- data.frame()

for (i in (1:length(multivariate_glms))){
  model <- multivariate_glms[[i]]
  
  model_log_lik <- log_lik(model)
  model_loo <- loo(model_log_lik)
  pareto_k_values <- append(pareto_k_values, 
                            list(model_loo$diagnostics$pareto_k))
  cat("\n-- Monitoring Model: ", mv_model_names[i], "-- \n")
  #print(model_loo)
  
  accs <- model_accuracies(model = multivariate_glms[[i]])
  
  aux_df <- data.frame("variable group" = mv_model_names[i],
                       "elpd_loo" = model_loo$estimates[1],
                       "p_loo" = model_loo$estimates[2],
                       "accuracy" = accs[[1]])
  
  multivariate_loo_estimates_df <- rbind(multivariate_loo_estimates_df, aux_df)
}

print(multivariate_loo_estimates_df)
```

All of the multivariate variable block models have relatively exceptional $\hat{k}$-values (lower than 0.7), with a few high exceptions (equal or higher than 0.7), which means that the results obtained by the models are relatively reliable.

For the `Full Variables Model` and `Three Variables Model`, we obtain relatively decent $\hat{k}$-values, with a few more high exceptions than for the variable block models. On the other hand, for the `RHS Model`, the $\hat{k}$-values are mediocre, as there is a high amount (14%) of higher than 0.7 values. Even though we obtain better PSIS-LOO and predictive results than for the previous models, we must be aware of the fact that `Full Variables Model`, `Three Variables Model` and `RHS Model` **may** be considered unreliable.

```{r}
p <- list()
for (i in 1:length(pareto_k_values)){
  df <- data.frame(pareto_k_values[[i]])
  colnames(df) <- c("values")
  p[[i]] <- ggplot(data=df, aes(y=values, x=1:nrow(cancer_data))) +
    geom_point(color="#10A5F5") +
    geom_hline(yintercept=0.7, linetype="dashed", size=1, color="#0859C6") +
    ggtitle(mv_model_names[i]) +  xlab("Samples") + ylab("K-values")
}
do.call(grid.arrange, c(p[1:3], ncol=3))
do.call(grid.arrange, c(p[4:6], ncol=3))
```

## Gaussian Model
When fitting the gaussian model with the four "optimal" variables, the results of the Pareto k values are, in general, good. However, for those variables for which the model yields a higher accuracy, the k values are larger, in some occasions, over 0.7. This might be representative of a model which is optimistically biased.
```{r}
pareto_k_values <- list()
linear_loo_estimates_df <- data.frame()

for (j in 1:(length(gaussian_fits))){
  cat("-Monitoring variable: ", variables_gaussian[j], "\n")
  model <- gaussian_fits[[j]]
  model_log_lik <- extract_log_lik(model, parameter_name="log_lik", 
                                   merge_chains=FALSE)
  model_r_eff <- relative_eff(exp(model_log_lik))
  model_loo <- loo(model_log_lik, r_eff = model_r_eff)
  pareto_k_values <- append(pareto_k_values, 
                            list(model_loo$diagnostics$pareto_k))
  
  aux_df <- data.frame("variable" = variables_gaussian[j], 
                       "elpd_loo" = model_loo$estimates[1], 
                       "p_loo" = model_loo$estimates[2])
  linear_loo_estimates_df <- rbind(linear_loo_estimates_df, aux_df)
}

cat("-- Gaussian Model Comparison")
print(linear_loo_estimates_df)
```

```{r}
p <- list()
for (i in 1:length(pareto_k_values)){
  df <- data.frame(pareto_k_values[[i]])
  colnames(df) <- c("values")
  p[[i]] <- ggplot(data=df, aes(y=values, x=1:N_predict)) +
    geom_point(color="#10A5F5") +
    geom_hline(yintercept=0.7, linetype="dashed", size=1, color="#0859C6") +
    ggtitle(variables_gaussian[i]) +  xlab("Samples") + ylab("K-values")
}
do.call(grid.arrange, c(p[1:3], ncol=3, nrow=1))
```
Finally, the auxiliary function for computing the accuracy of the model is implemented. The following function considers as accuracy the fraction of correctly labeled examples among all the predictions.

```{r}
accuracy_gaussian <- function(predictions, labels){
  trues <- 0
  falses <- 0
  for (i in 1:length(predictions)){
    if (predictions[i] == labels[i]){
      trues = trues +1
    }
    else{
      falses = falses +1
    }
  }
  return(trues/(falses+trues))
} 
```



# Predictive Performance Assessment

In order to decide whether the overall diagnosis ability of our model is reliable or not, we can take a look at the accuracy. Given the nature of the diagnosis: a negligence in this medical diagnostic could be harmful for a subject. Thus, we take into account the ratio of false positives and negatives. To accomplish this we visualize the confusion matrix produced by the function `confusionMatrix`. The output provides insightful measures for our data: sensitivity and specificity are statistical measures of the performance of a binary classification test that are widely used in medicine. Sensitivity measures the proportion of true positives that are correctly identified  whereas specificity measures the proportion of true negatives.

## Linear Models
The linear models were not assessed as thorough as the other approaches, as we already knew that the linear models weren't as useful, promising and reliable for diagnosing.

Given the previous linear model comparisons, the most reliable variables for diagnosing are the ones that have better accuracy compared to the rest as seen in the comparison table below.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Variable}   & \textbf{Accuracy} \\ \hline
14 - area\_worst    & 0.9104            \\ \hline
2 - area\_mean      & 0.8822            \\ \hline
8 - area\_se        & 0.8682            \\ \hline
4 - concavity\_mean & 0.8822            \\ \hline
\end{tabular}
\caption{Accuracy for most reliable linear models.}
\end{table}

As a final note on linear models, it must be reminded that these models mustn't be used for actual diagnosing as they aren't reliable.

## Multivariate Models

The multivariate model approach, as expected, is better for diagnosis prediction than the other approaches.

As seen in the table below, the best model is the `RHS Model` followed by the `Worst Model`. From the linear models analysis, it is inferred that the `SE` variables would perform worse, this is reflected in the results of the `SE Model`. 

Additionally, it can be seen how the projection predictive variable analysis carried out from the full model helped us achieve better results with lower amount of variables.

Moreover, a more thorough analysis can be found in [Appendix B - Multivariate models](#apb) in `Confusion Matrices`, where a confusion matrix is visualized for each model containing both the sensitivity and specificity. Generally, the predictions are reliable as the sensitivity and specificity for each model are good. On the other hand, one can see just how unreliable the `SE Model` is, as the number of false negatives are higher.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{variable.group} & \textbf{accuracy} & Sensitivity & Specificity \\ \hline
1 Mean Model            & 0.9438     & 0.9664 & 0.9057       \\ \hline
2 SE Model              & 0.8910 & 0.9580 & 0.7783             \\ \hline
3 Worst Model           & 0.9719 & 0.9888 & 0.9434           \\ \hline
4 Full Variables Model  & 0.9332 & 0.9608 & 0.8868            \\ \hline
5 Three variables Model & 0.9543 & 0.9720 & 0.9245            \\ \hline
6 RHS Model             & 0.9859 & 0.9944 & 0.9717           \\ \hline
\end{tabular}
\end{table}

## Gaussian Model
In the case of the gaussian processes, making use of the implemented function, the obtained accuracies are computed as:

```{r}
accuracies <- c()
for (j in 1:(length(gaussian_fits))){
  accuracy_gaussian_sum <- 0
  params_gaussian <- extract(gaussian_fits[[j]])
  for (i in 1:(dim(params_gaussian$y_predict)[1])){
    predictions <- params_gaussian$y_predict[i,1:N_predict]
    labels <- 
      test_set[,"diagnosis"][x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]]
    accuracy_gaussian_sum <- 
      accuracy_gaussian(predictions, labels) + accuracy_gaussian_sum
  }
  accuracy_gaussian_mean <- 
    accuracy_gaussian_sum/(dim(params_gaussian$y_predict)[1])
  accuracies <- c(accuracies, accuracy_gaussian_mean)
}
```

Yielding the following results:

```{r}
for (i in 1:length(variables_gaussian)){
  cat("\nGaussian model accuracy for ", variables_gaussian[i],
      " is: ", round(accuracies[i],3))
}
```

The results for the intermediate variables are not good at all. They improve for the first and last variable. However, they are far from the good results obtained with the other implemented models.


# Conclusion

In this project we carried out a bayesian data analysis for the dataset [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29), in order to detect if a subject's tumor sample, taken with Fine-needle Aspiration (FNA), is benign or malignant. This type of previous tumor dangerousness detection is important, as a major surgical biopsy can be avoided by simply performing FNA, a simple, safe and less traumatic procedure.

We proposed three different types of approaches, a linear one, a multivariate one and a gaussian one.

With the linear approach we concluded that the FNA analysis results termed "Standard Error" (i.e. texture_se, area_se, etc.), were the least important ones to take into account, but the linear conclusions shouldn't be taken too seriously, as in and of itself the FNA results shouldn't be taken into account individually.

For the multivariate approach, we decided to first to study each result type: "Mean", "Standard Error" and "Worst" (comprised of 6 variables each). This analysis provided us with the insight that FNA analysis results termed "Standard Error" were the least important ones as well, and that the ones termed "Worst" might be the most useful ones.

The second approach to multivariate was a more thorough one, we started by first analyzing all of the variables together, and from this we obtained that the optimal amount of variables to achieved the same or better results was 3. Additionally, we also got a ranking of which variables were better. With this optimal number of variables and the best ranked ones, we then obtained a more compact model which achieved better predictions for new data using regularized horseshoe prior.

AÑADIR DEL GAUSSIAN

## Issues and Improvements

Throughout the development of this project, some issues arose such as:

- The Full Multivariate Model had a few divergences present, and even though we managed to improve the model by trying to remove the divergences by adding more iterations and tweaking the `adapt_delta`, a handful (11 divergences out of 10000 iterations) still remained. Other attempts to solve the problem yielded in heavy computations for the model and were discarded.

- The Pareto-k values of the Regularized Horseshoe Multivariate Model had quite a few high valued $\hat{k}$-values (higher than 0.7), which may mean that the model could be unreliable. We did not have enough time to search for a solution to this problem.

## Group Self-Reflection

In hindsight, we gained an understanding of:

- The R language and using Stan and its libraries.

- Analysis and selection of relevant variables from a dataset.

- Different modeling approaches: i.e. Regularized Horseshoe Prior and Gaussian Model.

# References

- BDA3 Book

- Prior Choice Recommendations
https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

- Bodyfat
https://avehtari.github.io/modelselection/bodyfat.html

- Diabetes
https://avehtari.github.io/modelselection/diabetes.html

- Regularized Horseshoe Example
https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html

- Gaussian Process Stan Documentation
https://mc-stan.org/docs/2_19/stan-users-guide/gaussian-processes-chapter.html

- Gaussian Process Stan Video Tutorial
https://youtu.be/132s2B-mzBg

# Appendices
## Appendix A - Linear models {#apa}
### Monitor

Monitor output has been omitted for convenience, only relevant variables (Rhat, Bulk_ESS and Tail_ESS) are shown.

```{r}
for (i in 1:length(linear_models)){
  cat("\nMonitor for model variable:", variables[i], '\n')
  cat("-------------------------\n")
  model_mon <- monitor(linear_models[[i]], print=FALSE)
  rhat_alpha <- round(model_mon$Rhat[1], 3)
  rhat_beta <- round(model_mon$Rhat[2], 3)
  bulk_alpha <- round(model_mon$Bulk_ESS[1], 3)
  bulk_beta <- round(model_mon$Bulk_ESS[2], 3)
  tail_alpha <- round(model_mon$Tail_ESS[1], 3)
  tail_beta <- round(model_mon$Tail_ESS[2], 3)
  cat("Rhat of alpha and beta:", rhat_alpha, rhat_beta, '\n')
  cat("Bulk_ESS for alpha and beta:", bulk_alpha, bulk_beta, '\n')
  cat("Tail_ESS for alpha and beta:", tail_alpha, tail_beta, '\n')
  
}
```

### Divergences

```{r}
for (i in (1:length(linear_models))){
  cat("\n\n- Checking divergence for variable: ", variables[i], "\n")
  check_hmc_diagnostics(linear_models[[i]])
}
```


## Appendix B - Multivariate models {#apb}

### Monitor
Monitor output has been omitted for convenience, only relevant variables (Rhat, Bulk_ESS and Tail_ESS) are shown.

```{r}
for (i in 1:length(multivariate_glms)){
  cat("\nMonitor for multivariate model:", mv_model_names[i], '\n')
  cat("-------------------------\n")
  model_mon <- monitor(multivariate_glms[[i]]$stanfit, print=FALSE)
  rhat_alpha <- round(model_mon$Rhat[1], 3)
  rhat_beta <- round(model_mon$Rhat[2], 3)
  bulk_alpha <- round(model_mon$Bulk_ESS[1], 3)
  bulk_beta <- round(model_mon$Bulk_ESS[2], 3)
  tail_alpha <- round(model_mon$Tail_ESS[1], 3)
  tail_beta <- round(model_mon$Tail_ESS[2], 3)
  cat("Rhat of alpha and beta:", rhat_alpha, rhat_beta, '\n')
  cat("Bulk_ESS for alpha and beta:", bulk_alpha, bulk_beta, '\n')
  cat("Tail_ESS for alpha and beta:", tail_alpha, tail_beta, '\n')
}
```

### Divergences

```{r}
for (i in 1:length(multivariate_glms)){
 cat("Checking divergences for multivariate model:", mv_model_names[i], '\n')
 cat("-------------------------\n")
 check_hmc_diagnostics(multivariate_glms[[i]]$stanfit)
}
```

### Confusion Matrices

```{r}
for (i in 1:length(multivariate_glms)){
  accs <- model_accuracies(model = multivariate_glms[[i]])
  print(mv_model_names[i])
  print(accs[[2]])
}
```

## Appendix C - Gaussian model {#apc}

### Monitor
Monitor output has been omitted for convenience, only relevant variables (Rhat, Bulk_ESS and Tail_ESS) are shown.
```{r}
for (i in 1:length(gaussian_fits)){
  cat("\nMonitor for model variable:", variables_gaussian[i], '\n')
  cat("-------------------------\n")
  model_mon <- monitor(gaussian_fits[[i]], print=FALSE)
  rhat_rho <- round(model_mon$Rhat[1], 3)
  rhat_alpha <- round(model_mon$Rhat[2], 3)
  rhat_a <- round(model_mon$Rhat[3], 3)
  rhat_eta <- round(mean(model_mon$Rhat[4:(N_predict+N_obs)]), 3)
  
  bulk_rho <- round(model_mon$Bulk_ESS[1], 3)
  bulk_alpha <- round(model_mon$Bulk_ESS[2], 3)
  bulk_a <- round(model_mon$Bulk_ESS[3], 3)
  bulk_eta <- round(mean(model_mon$Bulk_ESS[4:N_predict+N_obs]), 3)
  
  tail_rho <- round(model_mon$Tail_ESS[1], 3)
  tail_alpha <- round(model_mon$Tail_ESS[2], 3)
  tail_a <- round(model_mon$Tail_ESS[3], 3)
  tail_eta <- round(mean(model_mon$Tail_ESS[4:N_predict+N_obs]), 3)
  
  
  cat("Rhat of rho, alpha, a and eta:", 
      rhat_rho, rhat_alpha, rhat_a, rhat_eta, '\n')
  cat("Bulk_ESS for rho, alpha, a and eta:", 
      bulk_rho, bulk_alpha, bulk_a, bulk_eta, '\n')
  cat("Tail_ESS for rho, alpha, a and eta:", 
      tail_rho, tail_alpha, tail_a, tail_eta, '\n')
  
}
```

### Markov chain traceplots
```{r, echo=FALSE}
traceplot(gaussian_fits[[1]], pars=c("rho", "alpha", "a"), nrow=3, ncol=1)
traceplot(gaussian_fits[[2]], pars=c("rho", "alpha", "a"), nrow=3, ncol=1)
traceplot(gaussian_fits[[3]], pars=c("rho", "alpha", "a"), nrow=3, ncol=1)
```

## Appendix D - Model Code {#apd}

### Multivariate Variable Block Stan Code
```{r}
writeLines(readLines("models/linear_model_bernoulli_multivariate.stan"))
```

### Multivariate Variable Block Stan_GLM Code

```{r}
get_stanmodel(multivariate_glms[[1]]$stanfit)
```

### Multivariate Full Model Stan_GLM Code

```{r}
get_stanmodel(multivariate_full$stanfit)
```

### Multivariate Three Variables Model Stan_GLM Code

```{r}
get_stanmodel(multivariate_three$stanfit)
```

### Multivariate RHS Model Stan_GLM Code

```{r}
get_stanmodel(multivariate_rhsp$stanfit)
```


