---
title: "Project BDA"
author: "Anonymous"
date: "11/14/2020"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '1'
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loaded packages

```{r, message=FALSE, warning=FALSE}
library(rstan) 
rstan_options(auto_write = TRUE)
options(mc.cores = 3)
library(ggplot2)
library(aaltobda)
library(shinystan)
library(bayesplot)
library(loo)
library(dlookr)
library(corrplot)
library(rstanarm)
library(projpred)
library(GGally)
library(shiny)
library(gridExtra)
library(caret)
library(e1071)
library(pROC)
SEED <- 48927
```



# Introduction

Breast cancer most commonly presents as a lump that feels different from the rest of the breast tissue. More than 80% of cases are discovered when a person detects such a lump with the fingertips and there are various methods of assessing if the detected lump is a cyst, and if so, either benign or malignant.

One such method of detecting the dangerousness of the mass is Fine-needle Aspiration (FNA). This diagnostic procedure consists of a very safe and minor procedure, where a thin hollow needle is inserted into the mass for obtaining cell samples and analyzing them under a microscope. A major surgical biopsy can be avoided by performing FNA, which is safer and far less traumatic, and possibly eliminating the need for hospitalization and other complications.

The problem presented in this report deals with finding out which features of a FNA are more relevant in diagnosing a patient's mammary lump as benign or malignant. The data used is the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29), a data set whose features are computed from digitized images of FNAs of breast mass.

## Main Modeling Idea

As a summary, our modeling idea has been to do the following analysis' with different types of models:

1. **Linear Model**, with each of the remaining 18 features, to see which individual variable might have more influence in predicting correct diagnosis.
2. **Multivariate Models**, done with 3 blocks of 6 variables from mean, se and worst, to see which variable block might have more influence in predicting correct diagnosis.
3. **Multivariate Model**, done with all 18 features in order to obtain proper posterior checking, and see the minimum optimal amount of variables needed, and which ones, for equal or better prediction as this model with all 18 features.
4. **Multivariate Model**, done with the best minimum optimal amount of variables, to check if the predictions obtained are equal or better than the model with all variables.
5. **Multivariate Model**, done with Regularized Horseshoe Prior with the best minimum optimal amount of variables, in order to check if the predictions obtained are equal or better than the model with all variables.
6. **Gaussian Model**, ...

# Exploratory Data Analysis

The breast cancer dataset consists of 569 FNA procedures each with 32 features. Ten real-valued features are computed for each cell nucleus:

a) radius
b) texture
c) perimeter
d) area
e) smoothness
f) compactness
g) concavity
h) concave points
i) symmetry
j) fractal dimension

From these, the mean, standard error and worst, are computed, thus making 10 features into 30. The other 2 features are the FNA ID number and the diagnosis, the target binary variable, which has values 'M' (malignant) or 'B' (benign).

```{r}
cancer_data = read.csv('cancer.csv')
head(cancer_data)
```

```{r}
nrows <- nrow(cancer_data)
ncols <- ncol(cancer_data)
cat("Breast cancer dataset size:", nrows, "rows x", ncols, "cols")
```

As an addendum when talking about the amount of features the dataset has, it can be noticed that instead of the 32 features mentioned, there are really 33 columns, this is because the last column, called 'X', is a pointless column present filled with 'N/A' values. This column will be dropped.

```{r}
cancer_data$X <- NULL
cat("Dataset contains NULL values:", any(is.na(cancer_data)), '\n')
```

In order to present our data into an numeric format, we convert the target output into binary values (B into '0' and M into '1')

```{r}
cancer_data[cancer_data == "B"] <- as.numeric(0)
cancer_data[cancer_data == "M"] <- as.numeric(1)
cancer_data$diagnosis <- as.numeric(as.character(cancer_data$diagnosis))
```

Is our data balanced? We would like to know more about how many benign and malignant tumors are contained in our dataset by visualizing a barplot.

```{r, fig.height=3, fig.width=4}
ggplot(cancer_data, aes(factor(diagnosis), fill=as.factor(diagnosis))) +
      geom_bar() + theme(legend.position="none") + xlab("Diagnosis") +
      ggtitle("Benign and malignant tumor")
```

A correlation matrix is visualized to obtain correlation coefficients between variables.

```{r}
corrplot(cor(cancer_data[,2:32]))
```

As seen in the correlation matrix, there are some variables that are *highly* correlated. Since they do not provide any new information, we could apply feature selection. For instance, `radius_mean`, `perimeter_mean` and `area_mean` are highly correlated, so we decide to keep `area_mean` in our dataset. Moreover, `compactness_mean`, `concavity_mean` and `concave.points_mean` are correlated, so we decide to keep `concavity_mean`. This process is also carried out for the standard deviation and worst related variables.

```{r}
# Dropping columns using the subset function. '-' indicates dropping vars. 
cancer_data = subset(cancer_data, select = -c(radius_mean, perimeter_mean, 
                     compactness_mean, concave.points_mean, radius_se, 
                     perimeter_se, compactness_se, concave.points_se, 
                     radius_worst, perimeter_worst, compactness_worst, 
                     concave.points_worst))

variables <- c("texture_mean", "area_mean", "smoothness_mean", "concavity_mean",
               "symmetry_mean", "fractal_dimension_mean", "texture_se",
               "area_se", "smoothness_se", "concavity_se", "symmetry_se",
               "fractal_dimension_se", "texture_worst", "area_worst", 
               "smoothness_worst", "concavity_worst", "symmetry_worst", 
               "fractal_dimension_worst")
```

By performing this feature selection, we drastically removed the number of variables, from 30 to 18. The final variables, along with the correlation, used for our models are as follows:

```{r}
corrplot(cor(cancer_data))
```

There are still some variables that are *slightly* correlated, but a more curated feature selection will be made later on in order to create our models.

Now, we would like to **scale** the columns of our data into the range [0,1] for easier comparison, except the diagnosis and id column. Thus, each column has a mean of 0 with a standard deviation of 1.

```{r}
scaled_cancer_data <- cancer_data
col_names <- colnames(cancer_data) # Keep column names

scaled_cancer_data[,1:2] <- cancer_data[,1:2] # Id and diagnosis without scaling
scaled_cancer_data[,3:18] <- scale(cancer_data[,3:18])
colnames(scaled_cancer_data) <- col_names # Retrieve column names
head(scaled_cancer_data)
```

# Priors

We have chosen to work with Weakly Informative Priors because:

1. A weakly informative prior means a reasonable representation of partial ignorance about the data, but that still includes a small amount of real-world information. Uniformity of distribution on an appropriate measurement scale means that the prior does not strongly favor particular values of the parameter.

2. A weakly informative prior does not contribute strongly to the posterior. With a weakly informative prior we say that this small contribution from the prior "lets the data speak for itself".

Because the target feature, the diagnosis, has binary values we will use a Bernoulli logistic regression approach: 

$$ y \sim Bernoulli(y\ |\ logit^{-1}(\alpha + \beta \times x)),$$

with $\alpha$ and $\beta$ are the intercept and regression coefficients, $x$ are the predictor variables and $y$ is the breast cancer diagnosis.

## Coefficients

For the $\alpha$ intercept coefficient and $\beta$ regression coefficient we have chosen a Normal and a Student T distribution respectively, to be more precise:

1. $\alpha \sim Normal(\mu_{\alpha},\ \sigma_{\alpha})$, with $\mu_{\alpha} = 0$ and $\sigma_{\alpha} = 1$.
2. $\beta \sim StudentT(df_{\beta},\ location_{\beta},\ scale_{\beta})$, with $df_{\beta} = 3$, $location_{\beta} = 0$ and $scale_{\beta} = 1$.

The reason behind this choice in priors for the intercept coefficient $\alpha$ is because we scale our data, so it makes sense to select a normal distribution with mean centered in 0 and standard deviation 1. On the other hand, for the regression coefficient $\beta$ we chose a StudenT distribution with 3 degrees of freedom and scale 1 because we want to ensure a finite variance and also a finite mean, which will create thick tails that reflect our uncertainty in the prior scale.

```{r}
beta <- student_t(df = 3, location = 0, scale = 1)
alpha <- normal(0, 1)
```

## Regularized Horseshoe Prior

We have also used a Weakly Regularized Horseshoe Prior, as *Piironen+Vehtari:RHS:2017*, where we included the prior assumption that a number of variables are more relevant that the rest. The assumption origin will be discussed in a future section, but as a summary, it has been assumed that there are 3 relevant variables, while the others are irrelevant.

$$
\begin{aligned}

\beta_m &\sim N(0,\ \tau \times\ \tilde\lambda) \\[2pt]
\tilde\lambda_m &\sim \frac{c \times \tilde\lambda_m}{\sqrt{c^2 + \tau^2\times \tilde\lambda_m^2}} \\[2pt]
\lambda_m &\sim HalfCauchy(0,\ 1) \\[2pt]
c^2 &\sim InvGamma(\frac{v}{2},\ \frac{v}{2}\times s^2)\\[2pt]
\tau &\sim HalfCauchy(0,\ \tau_0)\\[2pt]

\end{aligned}
$$

```{r}
p_0 <- 3 # number of relevant variables
tau_0 <- p_0 / (length(variables) - p_0) * 1 / sqrt(nrows)
rhs_prior <- hs(global_scale = tau_0)
```
## Gaussian Model
Gaussian processes are parametrized by a mean vector and a covariance matrix. The kernel that yields the covariance matrix depends on two hyperpriors, the length scale, $\rho$, and the marginal standard deviation $\alpha$. The mean vector prior was characterized by a standard normal distribution, as well as the other parameters, except for the length scale, which samples from an inverse gamma distribution.

# Models
Throughout this section, the designed modeling ideas will be presented in its respective subsection: explanation of the model, code, analysis and diagnostics.

## Linear Models
Per each variable contained in the data set, a separate linear model is created in order to observe its efficiency and relevance.

### Stan code
The general **stan code** is designed as follows:
```{r}
writeLines(readLines("models/linear_model_bernoulli.stan"))
linear_model <- rstan::stan_model(file = "models/linear_model_bernoulli.stan")
```
As seen in the stan model above, the priors are $\alpha \sim N(0,1)$ for the intercept coefficient and $\beta \sim StudentT(3,0,1)$ for the regression coefficient. The likelihood has been calculated using `bernoulli_logit` for the binary outcome. In order to compute the log-likelihood of the model `bernoulli_logit_lpmf` has been used as well as `bernoulli_logit_rng` to compute the predictions of the outcome.

### Sampling
The following code draws samples from the defined stan model. Each iteration pertains to a variable of the data set ('texture_mean', 'area_mean', etc...) contained in `variables` variable.

```{r}
linear_models <- c()
y <- c(scaled_cancer_data$diagnosis) # Binary outcomes
for (variable in variables){
  cat("Sampling with variable: ", variable, "\n")
  # Data
  linear_model_data <- list(N=nrows, y=y, x=c(scaled_cancer_data[,variable]))

  # Model sampling
  linear_sampling <- rstan::sampling(linear_model, data = linear_model_data,
                                     seed = SEED, refresh=0)

  # Model saved into a list for further analysis later on.
  linear_models <- c(linear_models, linear_sampling)
}
```




## Multivariate Model - Variable Blocks

Followed by the analysis done for each of the variables, it was decided to create three separate multivariate models, one for each variable block (type of variable): **mean**, **se** and **worst**. This was done in order to observe the efficiency and relevance over the type of the variable and see if some of the measure blocks shouldn't be considered when making the diagnosis.

### Stan code

```{r}
# Auxiliary function to calculate the accuracy of a model
model_accuracies <- function(model){
  y_pred <- posterior_epred(model)
  preds <- colMeans(y_pred)
  predicted <- predict(model, newdata=scaled_cancer_data, type='response')
  
  pr <- as.integer(preds >= 0.5)
  acc <- round(mean(xor(pr, as.integer(scaled_cancer_data$diagnosis==0))), 4)
  
  conf_matrix <- confusionMatrix(factor(round(predicted)), factor(scaled_cancer_data$diagnosis))
  return (list(acc, list(conf_matrix)))
}

writeLines(readLines("models/linear_model_bernoulli_multivariate.stan"))
linear_model_bernoulli_multivariate <- rstan::stan_model(file = "models/linear_model_bernoulli_multivariate.stan")
```

As seen in the stan model above, the priors are $\alpha \sim N(0,1)$ for the intercept coefficient and $\beta_n \sim StudentT(3,0,1)$ for each of the regression coefficients. The likelihood has been calculated using `bernoulli_logit` for the binary outcome. In order to compute the log-likelihood of the model `bernoulli_logit_lpmf` has been used as well as `bernoulli_logit_rng` to compute the predictions of the outcome.

### Sampling
The following code draws samples from the defined stan model. Each iteration pertains to a variable block ('mean', 'se' and 'worst'), comprised of 6 variables for each of them ('texture', 'area', 'smoothness', 'concavity', 'symmetry', 'fractal_dimension'), which are passed as predictors to the model.

Additionally to the `rstan::sampling` models obtained, the same models were fitted with `stan_glm`, in order to be able to make use of functions that need the input to be a stan_glm object. Both types of model objects have been saved for posteriority, for obtaining the needed diagnostics.

```{r}
mv_mean_block <- list("texture_mean", "area_mean", "smoothness_mean",
                      "concavity_mean", "symmetry_mean", 
                      "fractal_dimension_mean")

mv_se_block <- list("texture_se", "area_se", "smoothness_se", "concavity_se",
                       "symmetry_se", "fractal_dimension_se")

mv_worst_block <- list("texture_worst", "area_worst", "smoothness_worst",
                       "concavity_worst", "symmetry_worst", 
                       "fractal_dimension_worst")

mv_model_names <- c("Mean Model", "SE Model", "Worst Model", 
                    "Full Variables Model", "Three variables Model", 
                    "RHS Model")

mv_var_block <- list(mv_mean_block, mv_se_block, mv_worst_block)

multivariate_models <- c()
multivariate_glms <- list()

for (i in (1:3)){
  var_block <- mv_var_block[[i]]
  print(paste("diagnosis ~", paste(var_block, collapse = "+")))
  multivariate_block_data <- list(N=length(scaled_cancer_data$diagnosis), 
                               y=c(scaled_cancer_data$diagnosis), 
                               x_1=scaled_cancer_data[,var_block[[1]]],
                               x_2=scaled_cancer_data[,var_block[[2]]],
                               x_3=scaled_cancer_data[,var_block[[3]]],
                               x_4=scaled_cancer_data[,var_block[[4]]],
                               x_5=scaled_cancer_data[,var_block[[5]]],
                               x_6=scaled_cancer_data[,var_block[[6]]])
  multivariate_sampling <- rstan::sampling(linear_model_bernoulli_multivariate,
                                           data = multivariate_block_data,
                                           seed = SEED, refresh=0)
  multivariate_models <- c(multivariate_models, multivariate_sampling)
  
  multivariate_glm <- stan_glm(paste("diagnosis ~", paste(var_block, collapse = "+")),
                               data = scaled_cancer_data,
                               family = binomial(link = "logit"), 
                               prior = beta, prior_intercept = alpha, QR=TRUE,
                               seed = SEED, refresh=0)
  multivariate_glms <- append(multivariate_glms, list(multivariate_glm))
}
```

## Multivariate Model - Full Model

```{r}
multivariate_full <- stan_glm(diagnosis ~ texture_mean + area_mean + smoothness_mean + concavity_mean
                              + symmetry_mean + fractal_dimension_mean
                              + texture_se + area_se + smoothness_se + concavity_se
                              + symmetry_se + fractal_dimension_se
                              + texture_worst + area_worst + smoothness_worst + concavity_worst
                              + symmetry_worst + fractal_dimension_worst,
                              data = scaled_cancer_data,
                              family = binomial(link = "logit"), 
                              prior = beta, prior_intercept = alpha, QR=TRUE,
                              seed = SEED, refresh=0)
multivariate_glms <- append(multivariate_glms, list(multivariate_full))
```

## Multivariate Model - Three Variables Model

```{r}
multivariate_three <- stan_glm(diagnosis ~ area_worst + smoothness_worst + texture_mean,
                               data = scaled_cancer_data,
                               family = binomial(link = "logit"), 
                               prior = beta, prior_intercept = alpha,
                               seed = SEED, refresh=0)
multivariate_glms <- append(multivariate_glms, list(multivariate_three))
```

## Multivariate Model - Regularized Horseshoe Model

```{r}
multivariate_rhsp <- stan_glm(diagnosis ~ texture_mean + area_mean + smoothness_mean + concavity_mean
                              + symmetry_mean + fractal_dimension_mean
                              + texture_se + area_se + smoothness_se + concavity_se
                              + symmetry_se + fractal_dimension_se
                              + texture_worst + area_worst + smoothness_worst + concavity_worst
                              + symmetry_worst + fractal_dimension_worst,
                              data = scaled_cancer_data,
                              family = binomial(link = "logit"),
                              prior=rhs_prior, QR=TRUE,
                              seed = SEED, refresh=0)
multivariate_glms <- append(multivariate_glms, list(multivariate_rhsp))
```

## Gaussian Model
The last model that was considered for this project was a Gaussian Process based model. Gaussian models provide a probability distribution over functions. They are useful for representing those datasets that cannot be easily adjusted to a line following a regression procedure. A gaussian process consists of generating higher degree polynomial functions and assigning to those functions a probability. Therefore, by taking the mean over the probability distribution, the most probable fit is obtained.

$\linebreak$

In order to later assess the performance of the gaussian model based on the quality of its predictions, the data set is divided into two subsets, one for "training" the model, containing a higher number of observations, and another one for "testing" the model.

### Stan code

```{r}
train_index <- sample(1:nrow(scaled_cancer_data), 0.8*nrow(scaled_cancer_data))
test_index <- setdiff(1:nrow(scaled_cancer_data), train_index)

train_set <-scaled_cancer_data[train_index, ]
test_set <- scaled_cancer_data[test_index, ]
```

The gaussian model was carried out over the four best variables contained in the set of optimal variables obtained using the previous models.
```{r}
variables_gaussian <- c("area_worst", "smoothness_mean", "texture_mean", 
                        "area_mean")
gaussian_fits <- c()
x_predicts_idx <- c()
N_obs <- 200
N_predict <- 100
gaussian_model <- rstan::stan_model(file = "models/gaussian_process_bernoulli.stan")
```

From the training set, 200 randomly selected observations are considered. Gaussian models are computationally slow, hence, using the total number of observations would take much time. From the test set, 100 observations are chosen, also randomly.
```{r}
for (variable in variables_gaussian){
  cat("\nSampling from model with variable: ", variable, "\n")
  x_obs_idx <- sort(sample(1:length(train_set[,variable]), N_obs))
  x_predict_idx <- sort(sample(1:length(test_set[,variable]), N_predict))
  x_predicts_idx <- c(x_predicts_idx, x_predict_idx)
  gaussian_model_data <- list(N_predict = N_predict,
                              x_predict = test_set[,variable][x_predict_idx],
                              N_obs = N_obs,
                              x_obs = train_set[,variable][x_obs_idx],
                              y_obs = train_set[,"diagnosis"][x_obs_idx])
  
  gaussian_fit <-rstan::sampling(gaussian_model, data = gaussian_model_data,
                                 iter=1000, chains=2, refresh=0)
  gaussian_fits <- c(gaussian_fits, gaussian_fit)
}
```
Finally, an additional function for computing the accuracy of the model is implemented. The following function considers as accuracy the fraction of correctly labelled examples among all the predictions.
```{r}
accuracy_gaussian <- function(predictions, labels){
  trues <- 0
  falses <- 0
  for (i in 1:length(predictions)){
    if (predictions[i] == labels[i]){
      trues = trues +1
    }
    else{
      falses = falses +1
    }
  }
  return(trues/(falses+trues))
} 
```

# Convergence Diagnostics
## Diagnostic Metrics
The aim of this section is to assess the convergence of our models using a variety of diagnostics:

$\hat{R}$ can be used to monitor if a group of chains has converged to the target distribution. This can be done by comparing the between and within-chain estimate for model parameters. If chains have not converged, meaning that they have not mixed well, $\hat{R}$ will be greater than one. If chains have converged, the $\hat{R}$ value will be virtually one.

We would like to have a diagnostic that tell us when the weights are problematic, as we could have vastly imbalanced weights. In this case, we could use **effective sample size ($S_{eff}$)**, a quantitative measure of efficiency in **importance sampling**. $S_{eff}$ represents the number of independent samples required to obtain an importance sampling estimate with about the same efficiency as if we would have used all the samples.

A divergence arises when the simulated Hamiltonian trajectory departs from the true trajectory as measured by departure of the Hamiltonian value from its initial value. To evaluate this, we can use the function `check_hmc_diagnostics`.

## Linear Model Diagnostics
As seen in the `Appendix A`[Appendix A - Linear models] `monitor` outputs, all $\hat{R}$ values were 1 or close to 1, which means that the linear models' chains have converged.

The `monitor` function gives us two crude measures of effective sample size called Bulk_ESS and Tail_ESS (an $ESS > 100$ per chain is considered good). Additionally, our ESS values are way higher than 100, which established that our values are good.

Finally, we have obtained 0 divergences for all of our linear models.

## Multivariate Model Diagnostics
### Blocks

### Full Model

### Three Variables

### Regularized Horseshoe

## Gaussian Model Diagnostics
In order to visually assess convergence of the gaussian model, the chains for the scalar parameters are plotted.
```{r}
traceplot(gaussian_fits[[1]], pars=c("rho", "alpha", "a"), nrow=3, ncol=1)
traceplot(gaussian_fits[[2]], pars=c("rho", "alpha", "a"), nrow=3, ncol=1)
traceplot(gaussian_fits[[3]], pars=c("rho", "alpha", "a"), nrow=3, ncol=1)
traceplot(gaussian_fits[[4]], pars=c("rho", "alpha", "a"), nrow=3, ncol=1)
```

As in previous cases, the values obtained from applying the function `monitor`are satisfactory. $\hat{R}$ provides a valid scalar metric for convergence checking based on computing the division between the pooled variance of the chains and the individual variance of each of them. Obtaining values close to 1, as in the current case, means that the chains happen to have very similar distributions. Finally, the results yielded for the effective sample size are higher than 100.

# Posterior Predictive Checking
## Linear Models

```{r}
y_sims <- list()
for (model in linear_models){
  params <- extract(model)
  y_sims <- append(y_sims, list(params$y_sim[1:100,]))
}

ppc1 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[1]])
ppc2 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[2]])
ppc3 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[3]])
ppc4 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[4]])
ppc5 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[5]])
ppc6 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[6]])
ppc7 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[7]])
ppc8 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[8]])
ppc9 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[9]])
ppc10 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[10]])
ppc11 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[11]])
ppc12 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[12]])
ppc13 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[13]])
ppc14 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[14]])
ppc15 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[15]])
ppc16 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[16]])
ppc17 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[17]])
ppc18 <- ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[18]])

bayesplot_grid(ppc1, ppc2, ppc3, ppc4,
               subtitles = c("texture_mean", "area_mean", "smoothness_mean",
                             "concavity_mean"))
bayesplot_grid(ppc5, ppc6, ppc7, ppc8,
               subtitles = c("symmetry_mean", "fractal_dimension_mean",
                             "texture_se", "area_se"))
bayesplot_grid(ppc9, ppc10, ppc11, ppc12,
               subtitles = c("smoothness_se", "concavity_se", "symmetry_se",
                             "fractal_dimension_se"))
bayesplot_grid(ppc13, ppc14, ppc15, ppc16, subtitles = 
                 c("smoothness_worst", "concavity_worst", "smoothness_worst", 
                   "concavity_worst"))
bayesplot_grid(ppc17, ppc18, subtitles = c("symmetry_worst", 
                                           "fractal_dimension_worst"))

```

## Multivariate Models
### Blocks

In the posterior predictive checking for each of the variable blocks, it is noticeable how much less the `se` block can influence the fitting of the model and thus getting better predictive results with these variables might not be a viable option.

```{r}
y_sims <- list()
for (model in multivariate_models){
  params <- extract(model)
  y_sims <- append(y_sims, list(params$y_sim[1:100,]))
}

bayesplot_grid(ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[1]]), subtitles = c("mean block"))
bayesplot_grid(ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[2]]), subtitles = c("se block"))
bayesplot_grid(ppc_dens_overlay(scaled_cancer_data$diagnosis, y_sims[[3]]), subtitles = c("worst block"))
```

## Gaussian Model
The density of the diagnosis in the test set is printed in black color. Overlapping, the densities for the predictions of the first 50 iterations. This graph enables visual assessment of the quality of the predictions. Preliminarly, the results seem to be satisfactory.
```{r}
layout(mat = matrix(c(1,2,3,4), nrow = 2, ncol = 2),heights = c(1, 1,1,1), widths = c(1,1,1,1))
for (j in 1:(length(gaussian_fits))){
  params_gaussian <- extract(gaussian_fits[[j]])
  plot(density(test_set$diagnosis), xlim=c(-0.5, 1.5), ylim=c(0,2.5), main=variables_gaussian[j], ylab="density", xlab="", lwd=3)
  for (i in 1:50){
  lines(density(params_gaussian$y_predict[i,]), col="cyan")
  }
}
```

Another way to check if the predicted samples represent the reality is by directly plotting them (orange) and comparing with the target ones (grey). This is possible because the gaussian model is applied on values that are contained in the test set for the chosen variable. The concentration of points at 0 or 1 *y* values should be similar for the same values of *x*. In the given cases, by looking at the graphs, there is a very similar tendency between the real values and the simulated ones, meaning that the result is good.
```{r}
for (j in 1:(length(gaussian_fits))){
  params_gaussian <- extract(gaussian_fits[[j]])
  layout(mat = matrix(c(1,2), nrow = 1, ncol = 2),heights = c(1, 1), widths = c(1,1))
  plot(c(range(test_set[,variables_gaussian[j]][x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]])), c(-1,1.5), ty="n", main = variables_gaussian[j], xlab = variables_gaussian[j], ylab = "diagnosis")
  points(test_set[,variables_gaussian[j]][x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]], test_set[,"diagnosis"][x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]], col="grey", cex=1, pch=19)
  #points(scaled_cancer_data[,variables_gaussian[j]], scaled_cancer_data$diagnosis, col="grey")
  plot(c(range(test_set[,variables_gaussian[j]][x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]])), c(-1,1.5), ty="n", main = variables_gaussian[j], xlab = variables_gaussian[j], ylab = "predicted diagnosis")
  for (i in 1:3){
    points(test_set[,variables_gaussian[j]][x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]], params_gaussian$y_predict[i, 1:N_predict], col="orange", pch=3, cex=1)
  }
}
```

# Model comparisons
## Linear Models
By using **Leave-One-Out Cross Validation**, we obtain the PSIS-LOO estimates for each of the variables for the Linear Model for posterior model comparison.

Most of the models have exceptional k-values (lower than 0.7), which means that the results obtained by the models are reliable. Additionally by looking at the `elpd_loo` and `p_loo` values for each of the models, we can deduce that the best variables to be used in order for diagnosis are as follows (in order of best): `area_worst`, `area_mean`, `area_se`, `concavity_mean`.

```{r}
pareto_k_values <- list()
linear_loo_estimates_df <- data.frame()

for (i in (1:length(linear_models))){
  cat("- Monitoring variable: ", variables[i], "\n")
  model <- linear_models[[i]]
  model_log_lik <- extract_log_lik(model, parameter_name = "log_lik",
                                   merge_chains = FALSE)
  model_r_eff <- relative_eff(exp(model_log_lik))
  model_loo <- loo(model_log_lik, r_eff = model_r_eff)
  pareto_k_values <- append(pareto_k_values,
                            list(model_loo$diagnostics$pareto_k))

  aux_df <- data.frame("variable" = variables[i],
                       "elpd_loo" = model_loo$estimates[1],
                       "p_loo" = model_loo$estimates[2])

  linear_loo_estimates_df <- rbind(linear_loo_estimates_df, aux_df)
}

print(linear_loo_estimates_df)
```

The following output visualizes the Pareto-k values for each linear model.

```{r}
p <- list()
for (i in 1:length(pareto_k_values)){
  df <- data.frame(pareto_k_values[[i]])
  colnames(df) <- c("values")
  p[[i]] <- ggplot(data=df, aes(y=values, x=1:nrow(cancer_data))) +
    geom_point(color="#10A5F5") +
    geom_hline(yintercept=0.7, linetype="dashed", size=1, color="#0859C6") +
    ggtitle(variables[i]) +  xlab("Samples") + ylab("K-values")
}
do.call(grid.arrange, c(p[1:4], ncol=2))
do.call(grid.arrange, c(p[5:8], ncol=2))
do.call(grid.arrange, c(p[9:12], ncol=2))
do.call(grid.arrange, c(p[13:16], ncol=2))
do.call(grid.arrange, c(p[17:18], ncol=2))
```
## Multivariate Models
### Blocks

By using **Leave-One-Out Cross Validation**, we obtain the PSIS-LOO estimates for each of the variable blocks for the Multivariate Model for posterior model comparison.

All of the models have relatively exceptional k-values (lower than 0.7), with a few high exceptions (equal or higher than 0.7), which means that the results obtained by the models are reliable. Additionally by looking at the `elpd_loo` and `p_loo` values for each of the models, we can deduce that the best variables to be used in order for diagnosis are as follows (in order of best): `worst block`, `mean block` and `se block`.

What we take out of this analysis by variable blocks is that, in general, the `se` variables might not be as influential for predictions as the other blocks. Although this analysis did not bring a more specific result, it was an interesting experiment to carry out in order to check if any of the variable blocks might influence too badly the final model.

Finally, it **must** be mentioned that the difference in prediction accuracy OF the `se block` and the `mean block` are ... .

```{r}
pareto_k_values <- list()
multivariate_loo_estimates_df <- data.frame()

for (i in (1:length(multivariate_models))){
  cat("- Monitoring variable group: ", mv_model_names[i], "\n")
  model <- multivariate_models[[i]]
  model_log_lik <- extract_log_lik(model, parameter_name = "log_lik", merge_chains = FALSE)
  model_r_eff <- relative_eff(exp(model_log_lik))
  model_loo <- loo(model_log_lik, r_eff = model_r_eff)
  pareto_k_values <- append(pareto_k_values, list(model_loo$diagnostics$pareto_k))
  
  accs <- model_accuracies(model = multivariate_glms[[i]])
  
  aux_df <- data.frame("variable group" = mv_model_names[i],
                       "elpd_loo" = model_loo$estimates[1],
                       "p_loo" = model_loo$estimates[2],
                       "accuracy" = accs[1])
  
  multivariate_loo_estimates_df <- rbind(multivariate_loo_estimates_df, aux_df)
}

print(multivariate_loo_estimates_df)
```

```{r}
p <- list()
block_name <- c("mean block", "se block", "worst block")
for (i in 1:length(pareto_k_values)){
  df <- data.frame(pareto_k_values[[i]])
  colnames(df) <- c("values")
  p[[i]] <- ggplot(data=df, aes(y=values, x=1:nrow(cancer_data))) +
    geom_point(color="#10A5F5") +
    geom_hline(yintercept=0.7, linetype="dashed", size=1, color="#0859C6") +
    ggtitle(block_name[i]) +  xlab("Samples") + ylab("K-values")
}

p[1]
p[2]
p[3]
```

### Full Model

### Three Variables

### Regularized Horseshoe

## Gaussian Model
When fitting the gaussian model with the four "optimal" variables, the results of the Pareto k values are, in general, good. However, for those variables for which the model yields a higher accuracy, the k values are larger, in some occasions, over 0.7. This might be representative of a model which is optimistically biased.
```{r}
pareto_k_values <- list()
linear_loo_estimates_df <- data.frame()

for (j in 1:(length(gaussian_fits))){
  cat("-Monitoring variable: ", variables_gaussian[j], "\n")
  model <- gaussian_fits[[j]]
  model_log_lik <- extract_log_lik(model, parameter_name="log_lik", merge_chains=FALSE)
  model_r_eff <- relative_eff(exp(model_log_lik))
  model_loo <- loo(model_log_lik, r_eff = model_r_eff)
  pareto_k_values <- append(pareto_k_values, list(model_loo$diagnostics$pareto_k))
  
  aux_df <- data.frame("variable" = variables_gaussian[j], "elpd_loo" = model_loo$estimates[1], "p_loo" = model_loo$estimates[2])
  linear_loo_estimates_df <- rbind(linear_loo_estimates_df, aux_df)
}

cat("-- Gaussian Model Comparison")
print(linear_loo_estimates_df)
```

```{r}
p <- list()
for (i in 1:length(pareto_k_values)){
  df <- data.frame(pareto_k_values[[i]])
  colnames(df) <- c("values")
  p[[i]] <- ggplot(data=df, aes(y=values, x=1:N_predict)) +
    geom_point(color="#10A5F5") +
    geom_hline(yintercept=0.7, linetype="dashed", size=1, color="#0859C6") +
    ggtitle(variables_gaussian[i]) +  xlab("Samples") + ylab("K-values")
}
do.call(grid.arrange, c(p[1:4], ncol=2, nrow=2))
```

# Predictive Performance Assessment
In order to decide whether the overall diagnosis ability of our model is reliable or not, we can take a look at the accuracy. Given the nature of the diagnosis, that a negligence in this medical diagnostic could be harmful for a subject. Thus, we take into account the ratio of false positives and negatives. To accomplish this we visualize the confusion matrix produced by the function `confusionMatrix`. The output provides insightful measures for our data: sensitivity and specificity are statistical measures of the performance of a binary classification test that are widely used in medicine: Sensitivity measures the proportion of true positives that are correctly identified  whereas specificity measures the proportion of true negatives.

[Decir si los valores predicción, sensitvity y specificity son buenos]

```{r}
for (i in 1:length(multivariate_glms)){
  accs <- model_accuracies(model = multivariate_glms[[i]])
  print(mv_model_names[i])
  print(accs[[2]])
}
```
In the case of the gaussian processes, making use of the implemented function, the obtained accuracies are computed as:
```{r}
accuracies <- c()
for (j in 1:(length(gaussian_fits))){
  accuracy_gaussian_sum <- 0
  params_gaussian <- extract(gaussian_fits[[j]])
  for (i in 1:(dim(params_gaussian$y_predict)[1])){
    predictions <- params_gaussian$y_predict[i,1:N_predict]
    labels <- test_set[,"diagnosis"][x_predicts_idx[((j-1)*N_predict+1):(j*N_predict)]]
    accuracy_gaussian_sum <- accuracy_gaussian(predictions, labels) + accuracy_gaussian_sum
  }
  accuracy_gaussian_mean <- accuracy_gaussian_sum/(dim(params_gaussian$y_predict)[1])
  accuracies <- c(accuracies, accuracy_gaussian_mean)
}
```
Yielding the following results:
```{r}
for (i in 1:length(variables_gaussian)){
  cat("\nGaussian model accuracy for ", variables_gaussian[i], " is: ", round(accuracies[i],3))
}
```
The results for the intermediate variables are not good at all. They improve for the first and last variable. However, they are far from the good results obtained with the other implemented models.


# Conclusion

## Issues and Improvements

- no poder usar stan::sampling con metodos de stan_glm
- super incomodo utilizar R para trabajar con datos
- 

## Group Self-Reflection



# References


# Appendices
## Appendix A - Linear models
### Monitor

Monitor output has been omitted for convenience, only relevant variables (Rhat, Bulk_ESS and Tail_ESS) are shown.

```{r}
for (i in 1:length(linear_models)){
  cat("\nMonitor for model variable:", variables[i], '\n')
  cat("-------------------------\n")
  model_mon <- monitor(linear_models[[i]], print=FALSE)
  rhat_alpha <- round(model_mon$Rhat[1], 3)
  rhat_beta <- round(model_mon$Rhat[2], 3)
  bulk_alpha <- round(model_mon$Bulk_ESS[1], 3)
  bulk_beta <- round(model_mon$Bulk_ESS[2], 3)
  tail_alpha <- round(model_mon$Tail_ESS[1], 3)
  tail_beta <- round(model_mon$Tail_ESS[2], 3)
  cat("Rhat of alpha and beta:", rhat_alpha, rhat_beta, '\n')
  cat("Bulk_ESS for alpha and beta:", bulk_alpha, bulk_beta, '\n')
  cat("Tail_ESS for alpha and beta:", tail_alpha, tail_beta, '\n')
  
}
```

### Divergences

```{r}
for (i in (1:length(linear_models))){
  cat("\n\n- Checking divergence for variable: ", variables[i], "\n")
  check_hmc_diagnostics(linear_models[[i]])
}
```


## Appendix B - Multivariate models
### Monitor
Monitor output has been omitted for convenience, only relevant variables (Rhat, Bulk_ESS and Tail_ESS) are shown.

```{r}
for (i in 1:length(multivariate_glms)){
  cat("\nMonitor for multivariate model:", mv_model_names[i], '\n')
  cat("-------------------------\n")
  model_mon <- monitor(multivariate_glms[[i]]$stanfit, print=FALSE)
  rhat_alpha <- round(model_mon$Rhat[1], 3)
  rhat_beta <- round(model_mon$Rhat[2], 3)
  bulk_alpha <- round(model_mon$Bulk_ESS[1], 3)
  bulk_beta <- round(model_mon$Bulk_ESS[2], 3)
  tail_alpha <- round(model_mon$Tail_ESS[1], 3)
  tail_beta <- round(model_mon$Tail_ESS[2], 3)
  cat("Rhat of alpha and beta:", rhat_alpha, rhat_beta, '\n')
  cat("Bulk_ESS for alpha and beta:", bulk_alpha, bulk_beta, '\n')
  cat("Tail_ESS for alpha and beta:", tail_alpha, tail_beta, '\n')
  
}
```
### Divergences
```{r}
for (i in 1:length(multivariate_models)){
 cat("Checking divergences for multivariate model:", mv_model_names[i], '\n')
 cat("-------------------------\n")
 check_hmc_diagnostics(multivariate_models[[i]])
}
```

## Appendix C - Gaussian model

### Monitor
Monitor output has been omitted for convenience, only relevant variables (Rhat, Bulk_ESS and Tail_ESS) are shown.
```{r}
for (i in 1:length(gaussian_fits)){
  cat("\nMonitor for model variable:", variables_gaussian[i], '\n')
  cat("-------------------------\n")
  model_mon <- monitor(gaussian_fits[[i]], print=FALSE)
  rhat_rho <- round(model_mon$Rhat[1], 3)
  rhat_alpha <- round(model_mon$Rhat[2], 3)
  rhat_a <- round(model_mon$Rhat[3], 3)
  rhat_eta <- round(mean(model_mon$Rhat[4:(N_predict+N_obs)]), 3)
  
  bulk_rho <- round(model_mon$Bulk_ESS[1], 3)
  bulk_alpha <- round(model_mon$Bulk_ESS[2], 3)
  bulk_a <- round(model_mon$Bulk_ESS[3], 3)
  bulk_eta <- round(mean(model_mon$Bulk_ESS[4:N_predict+N_obs]), 3)
  
  tail_rho <- round(model_mon$Tail_ESS[1], 3)
  tail_alpha <- round(model_mon$Tail_ESS[2], 3)
  tail_a <- round(model_mon$Tail_ESS[3], 3)
  tail_eta <- round(mean(model_mon$Tail_ESS[4:N_predict+N_obs]), 3)
  
  
  cat("Rhat of rho, alpha, a and eta:", rhat_rho, rhat_alpha, rhat_a, rhat_eta, '\n')
  cat("Bulk_ESS for rho, alpha, a and eta:", bulk_rho, bulk_alpha, bulk_a, bulk_eta, '\n')
  cat("Tail_ESS for rho, alpha, a and eta:", tail_rho, tail_alpha, tail_a, tail_eta, '\n')
  
}
```










